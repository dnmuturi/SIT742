{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnmuturi/SIT742/blob/main/SIT742Task2code_test_NM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**END TERM ASSIGNMENT**\n",
        "\n",
        " **GROUP C30 Members**\n",
        "\n",
        "1. David Muturi - S225177509\n",
        "2. Nhlanhla Matukane - S225177376\n",
        "3. Vincent Nwobi - S225177483"
      ],
      "metadata": {
        "id": "SzkIZK2xplg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 1**"
      ],
      "metadata": {
        "id": "cdkp7fJMbMMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.1**"
      ],
      "metadata": {
        "id": "G0MnsqpJbQ5r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53955062",
        "outputId": "f746d2c8-5329-4cb7-eccc-ac570d575796"
      },
      "source": [
        "#install the spark library\n",
        "!pip install pyspark\n",
        "!apt-get install pandoc"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pandoc is already the newest version (2.9.2.1-3ubuntu2).\n",
            "pandoc set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load necessary libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, from_unixtime, to_date, when, lit, trim, count, date_format,avg,struct, asc\n",
        "from pyspark.sql.functions import lower, regexp_replace, split, explode, year, concat_ws, length, collect_list,rank,countDistinct, month, dayofweek\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import hash\n",
        "import re\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from itertools import product"
      ],
      "metadata": {
        "id": "C4Ktnb9OqiWo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Define the path to the CSV file\n",
        "csv_path = '/content/drive/My Drive/SIT742/review.csv'\n",
        "# Define the path to the second CSV file\n",
        "meta_csv_path = '/content/drive/My Drive/SIT742/meta-review-business.csv'"
      ],
      "metadata": {
        "id": "AVlsAvUcoqkR",
        "outputId": "acdf3b7b-741d-4666-e6a2-622825cdad1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-869994131.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define the path to the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/SIT742/review.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the path to the second CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Once uploaded, the files exist on the server, and you can now use their names directly\n",
        "csv_path = 'review.csv'\n",
        "meta_csv_path = 'meta-review-business.csv'\n",
        "\n",
        "# Now you can load them\n",
        "df_reviews = pd.read_csv(csv_path)\n",
        "df_meta = pd.read_csv(meta_csv_path)\n",
        "\n",
        "print(\"\\nFiles successfully uploaded and loaded!\")\n",
        "df_reviews.head()\n",
        "df_meta.head()"
      ],
      "metadata": {
        "id": "WqRwooVEeYUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReviewAnalysis\").getOrCreate()\n",
        "\n",
        "# --- Load review.csv ---\n",
        "print(\"Loading 'review.csv' into a Spark DataFrame...\")\n",
        "try:\n",
        "    # Use the path from your file upload cell: 'review.csv'\n",
        "    df_review = spark.read.csv(\n",
        "        csv_path,\n",
        "        header=True,\n",
        "        inferSchema=True,\n",
        "        sep=',',\n",
        "        quote='\"',\n",
        "        escape='\"',\n",
        "        multiLine=True\n",
        "    )\n",
        "    print(\"review.csv' loaded successfully!\")\n",
        "    df_review.show(5, truncate=False)  # Display the first five rows without truncating\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading 'review.csv': {e}\")\n",
        "\n",
        "\n",
        "# --- Load meta-review-business.csv ---\n",
        "print(\"\\nLoading 'meta-review-business.csv' into a Spark DataFrame...\")\n",
        "try:\n",
        "    # Use the path from your file upload cell: 'meta-review-business.csv'\n",
        "    df_meta= spark.read.csv(\n",
        "        meta_csv_path,\n",
        "        header=True,\n",
        "        inferSchema=True,\n",
        "        sep=',',\n",
        "        quote='\"',\n",
        "        escape='\"',\n",
        "        multiLine=True\n",
        "    )\n",
        "    print(\"meta-review-business.csv' loaded successfully!\")\n",
        "    df_meta.show(5, truncate=False)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading 'meta-review-business.csv': {e}\")"
      ],
      "metadata": {
        "id": "fTGb3fMze0TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation for df_review\n",
        "print(\"--- DataFrame: df_review ---\")\n",
        "total_reviews = df_review.count()\n",
        "print(f\"Total rows: {total_reviews:,}\")\n",
        "print(\"\\nSchema:\")\n",
        "df_review.printSchema()"
      ],
      "metadata": {
        "id": "O0c_ydRPs_3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show summary statistics\n",
        "df_review.describe().show()"
      ],
      "metadata": {
        "id": "X0bNcdeKJ3Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation for df_meta\n",
        "print(\"\\n\\n DataFrame: df_meta\")\n",
        "total_businesses = df_meta.count()\n",
        "print(f\"Total rows: {total_businesses:,}\")\n",
        "print(\"\\nSchema:\")\n",
        "df_meta.printSchema()"
      ],
      "metadata": {
        "id": "-XavS7cCtJos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation for df_review\n",
        "print(\"--- DataFrame: df_review ---\")\n",
        "total_reviews = df_review.count()\n",
        "print(f\"Total rows: {total_reviews:,}\")\n",
        "print(\"\\nSchema:\")\n",
        "df_review.printSchema()"
      ],
      "metadata": {
        "id": "XshvR9_ms3hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Statistics for df_meta ---\n",
        "print(\"\\n Summary Statistics for df_meta\")\n",
        "df_meta.describe().show()"
      ],
      "metadata": {
        "id": "34PcdbfFtPbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before any further analysis, we evaluated the missing values in the dataset and used the Spark DataFrame functions to count them. The normal way to deal with missing values is to treat nulls as missing, empty strings as missing for text, and NaN as missing for numbers (Little and Rubin, 2019; van Buuren, 2018).\n",
        "# We do this with Apache Spark SQL functions and data types. The meanings of isNull, isnan, and trim are in the Spark docs.These are consistent with SQL-style null handling and are in line with how SQL handles nulls (Apache Spark, 2025).\n",
        "\n",
        "from pyspark.sql.functions import col, trim, when, isnan, lit, sum as F_sum, count as F_count\n",
        "from pyspark.sql.types import StringType, NumericType\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def missing_profile_spark(df, df_name=\"dataframe\"):\n",
        "    \"\"\"\n",
        "    Build a tidy table with missingness and completeness per column for a Spark DataFrame.\n",
        "    Returns a pandas.DataFrame for easy viewing and saving.\n",
        "    \"\"\"\n",
        "    # Basic checks\n",
        "    total_rows = df.count()\n",
        "    if total_rows == 0:\n",
        "        print(f\"[{df_name}] has 0 rows. Nothing to profile.\")\n",
        "        return pd.DataFrame(columns=[\"column\", \"dtype\", \"missing_count\", \"missing_rate\",\n",
        "                                     \"non_missing_count\", \"completeness_rate\"])\n",
        "\n",
        "    # Build \"missing flag\" per column in one pass to be efficient\n",
        "    cols = []\n",
        "    dtypes = []\n",
        "    missing_flags = []\n",
        "\n",
        "    for f in df.schema.fields:\n",
        "        c = f.name\n",
        "        dt = f.dataType\n",
        "        cols.append(c)\n",
        "        dtypes.append(str(dt))\n",
        "\n",
        "        # Define what counts as missing for this column\n",
        "        is_null = col(c).isNull()\n",
        "        if isinstance(dt, StringType):\n",
        "            is_blank = (trim(col(c)) == \"\")\n",
        "            flag = when(is_null | is_blank, 1).otherwise(0)\n",
        "        elif isinstance(dt, NumericType):\n",
        "            flag = when(is_null | isnan(col(c)), 1).otherwise(0)\n",
        "        else:\n",
        "            # For non-string, non-numeric types, we count only null as missing\n",
        "            flag = when(is_null, 1).otherwise(0)\n",
        "\n",
        "        missing_flags.append(F_sum(flag).alias(c))\n",
        "\n",
        "    # Aggregate all missing counts in one go\n",
        "    counts_row = df.agg(*missing_flags).collect()[0].asDict()\n",
        "\n",
        "    # Build a tidy pandas table\n",
        "    rows = []\n",
        "    for c, dt in zip(cols, dtypes):\n",
        "        miss = int(counts_row.get(c, 0))\n",
        "        miss_rate = miss / float(total_rows)\n",
        "        non_miss = total_rows - miss\n",
        "        comp_rate = 1.0 - miss_rate\n",
        "        rows.append({\n",
        "            \"column\": c,\n",
        "            \"dtype\": dt,\n",
        "            \"missing_count\": miss,\n",
        "            \"missing_rate\": round(miss_rate, 6),\n",
        "            \"non_missing_count\": non_miss,\n",
        "            \"completeness_rate\": round(comp_rate, 6)\n",
        "        })\n",
        "\n",
        "    out = pd.DataFrame(rows).sort_values([\"missing_rate\", \"missing_count\", \"column\"], ascending=[False, False, True]).reset_index(drop=True)\n",
        "\n",
        "    # Print a compact summary and return the full table\n",
        "    print(f\"\\n[{df_name}] rows: {total_rows}, columns: {len(cols)}\")\n",
        "    print(\"Top columns by missing_rate:\")\n",
        "    display(out.head(10))\n",
        "    return out\n",
        "\n",
        "# Run profiles for both data sets\n",
        "review_profile = missing_profile_spark(df_review, df_name=\"df_review\")\n",
        "meta_profile   = missing_profile_spark(df_meta,   df_name=\"df_meta\")\n",
        "\n",
        "\n",
        "# References (Harvard style):\n",
        "# Apache Spark (2025) Spark SQL, Built-in Functions. Available at: https://spark.apache.org/docs/latest/api/python/ (Accessed: 03/10/2025).\n",
        "# Little, R.J.A. and Rubin, D.B. (2019) Statistical Analysis with Missing Data. 3rd edn. Hoboken, NJ: Wiley.\n",
        "# van Buuren, S. (2018) Flexible Imputation of Missing Data. 2nd edn. Boca Raton, FL: CRC Press. Available at: https://stefvanbuuren.name/fimd/ (Accessed: 03/10/2025)."
      ],
      "metadata": {
        "id": "JSohcDMQvZmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_review — missingness bar plot\n",
        "# Idea:\n",
        "# - Convert a small slice from Spark to pandas (for plotting).\n",
        "# - Treat empty strings \"\" as missing by converting them to NaN.\n",
        "# - Plot a single missingness bar chart to see gaps per column.\n",
        "\n",
        "!pip -q install missingno\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "\n",
        "# Convert up to N rows to pandas\n",
        "N =  521515  # number of observations\n",
        "pdf_review = df_review.limit(N).toPandas()\n",
        "\n",
        "# Empty strings -> NaN so they count as missing for text columns\n",
        "pdf_review = pdf_review.replace(r\"^\\s*$\", np.nan, regex=True)\n",
        "\n",
        "# One clear plot\n",
        "plt.figure()\n",
        "msno.bar(pdf_review)\n",
        "plt.title(\"df_review - missingness by column (bar)\")\n",
        "plt.show()\n",
        "\n",
        "# Quick text summary (top 10 by missing rate)\n",
        "print(\"\\nTop 10 columns by missing rate (sampled):\")\n",
        "print(pdf_review.isna().mean().sort_values(ascending=False).head(10).round(3))\n"
      ],
      "metadata": {
        "id": "I5aP3oPB30X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_meta — missingness bar plot\n",
        "# Same approach as df_review to keep things consistent.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "\n",
        "# Convert up to N rows to pandas\n",
        "N = 12774 # number of observations\n",
        "pdf_meta = df_meta.limit(N).toPandas()\n",
        "\n",
        "# Empty strings -> NaN\n",
        "pdf_meta = pdf_meta.replace(r\"^\\s*$\", np.nan, regex=True)\n",
        "\n",
        "# One clear plot\n",
        "plt.figure()\n",
        "msno.bar(pdf_meta)\n",
        "plt.title(\"df_meta - missingness by column (bar)\")\n",
        "plt.show()\n",
        "\n",
        "# Quick text summary (top 10 by missing rate)\n",
        "print(\"\\nTop 10 columns by missing rate (sampled):\")\n",
        "print(pdf_meta.isna().mean().sort_values(ascending=False).head(10).round(3))\n",
        "\n",
        "# References (Harvard style):\n",
        "# Bilogur, A. (2018) missingno: a missing data visualization suite. Available at: https://github.com/ResidentMario/missingno (Accessed: 04/10/2025).\n",
        "# Little, R.J.A. and Rubin, D.B. (2019) Statistical Analysis with Missing Data. 3rd edn. Hoboken, NJ: Wiley.\n",
        "# van Buuren, S. (2018) Flexible Imputation of Missing Data. 2nd edn. Boca Raton, FL: CRC Press. Available at: https://stefvanbuuren.name/fimd/ (Accessed: 04/10/2025).\n"
      ],
      "metadata": {
        "id": "P3Sxs8A_3-ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.1.1**"
      ],
      "metadata": {
        "id": "oRj2lOE-raGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to count rows that are none or null\n",
        "def count_empty_rows(df):\n",
        "  # Filter rows where 'text' column is null or an empty string\n",
        "  null_or_empty_text_count = df.filter(\n",
        "    col(\"text\").isNull() | (trim(col(\"text\")) == \"\")\n",
        "  ).count()\n",
        "  return null_or_empty_text_count\n",
        "#show count of empty rows in text field before filling in with no review\n",
        "print(f\"The number of empty rows in the text column before replacing with 'no review' is: {count_empty_rows(df_review)}\")"
      ],
      "metadata": {
        "id": "4nA-TOEw_6yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace null values with \"No review\"\n",
        "df_review = df_review.fillna({'text': 'no review'})\n",
        "\n",
        "# Replace empty strings (after trimming whitespace) with \"No review\"\n",
        "df_review = df_review.withColumn(\"text\",\n",
        "    when(trim(col(\"text\")) == \"\", lit(\"no review\")).otherwise(col(\"text\"))\n",
        ")\n",
        "\n",
        "# Show the count of text column which is empty or null to verify the changes\n",
        "print(f\"The number of empty rows in the text column after replacing with no review is: {count_empty_rows(df_review)}\")\n",
        "\n",
        "# Count rows where the 'text' column is 'no review'\n",
        "no_review_count = df_review.filter(col(\"text\") == \"no review\").count()\n",
        "\n",
        "print(f\"The number of reviews with 'no review' in the text column is: {no_review_count}\")\n"
      ],
      "metadata": {
        "id": "NE7YPNsBmNDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.1.2**"
      ],
      "metadata": {
        "id": "PGG6r7-otTdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the legacy time parser policy to corrected to handle potential parsing issues\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
        "\n",
        "# Convert 'time' from epoch milliseconds to a timestamp, then to a date string\n",
        "df_review = df_review.withColumn(\n",
        "    \"newtime\",\n",
        "    to_date(from_unixtime(col(\"time\") / 1000).cast(\"timestamp\"), \"yyyy-MM-dd\")\n",
        ")\n",
        "\n",
        "# Display the first few rows with the new column\n",
        "df_review.select(\"user_id\",\"name\",\"rating\",\"time\", \"newtime\",\"text\",\"gmap_id\").show(5)"
      ],
      "metadata": {
        "id": "e702ILlLH2oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2**"
      ],
      "metadata": {
        "id": "s9SOA54GxrVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2.1**"
      ],
      "metadata": {
        "id": "73LlzorL555T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of reviews per unique gmap_id\n",
        "reviews_per_gmap = df_review.groupBy(\"gmap_id\").agg(count(\"*\").alias(\"review_count\"))\n",
        "\n",
        "# Cast the review_count to float type\n",
        "reviews_per_gmap = reviews_per_gmap.withColumn(\"review_count\", col(\"review_count\").cast(DoubleType()))\n",
        "\n",
        "# Show the top 5 results\n",
        "print(\"Number of reviews per unique gmap_id (Top 5):\")\n",
        "reviews_per_gmap.orderBy(col(\"review_count\").desc()).show(5)"
      ],
      "metadata": {
        "id": "aPzG0khxewGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2.2**"
      ],
      "metadata": {
        "id": "smGy0nJf6Akh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert PySpark DataFrame to pandas DataFrame\n",
        "df = df_review.toPandas()\n",
        "\n",
        "# Create 'review_time' column at the hour level\n",
        "df['review_time'] = pd.to_datetime(df['time'], unit='ms').dt.hour\n",
        "\n",
        "# Display the first 5 rows of the pandas DataFrame\n",
        "print(\"Pandas DataFrame with 'review_time' column (Top 5):\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "GKKiQDDB4JRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2.3**"
      ],
      "metadata": {
        "id": "br0XEAsR6DPr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2082982c"
      },
      "source": [
        "# Analyze the distribution of reviews by hour\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='review_time', palette='viridis')\n",
        "plt.title('Distribution of Review Times by Hour')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "\n",
        "# Analyze the number of businesses reviewed at different times\n",
        "unique_gmap_ids_per_hour = df.groupby('review_time')['gmap_id'].nunique().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=unique_gmap_ids_per_hour, x='review_time', y='gmap_id', palette='viridis')\n",
        "plt.title('Number of Unique Businesses Reviewed per Hour')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Number of Unique Businesses')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe1c49ca"
      },
      "source": [
        "### Insights from Visualizations\n",
        "\n",
        "The visualizations above provide insights into the review patterns based on the time of day:\n",
        "\n",
        "**Distribution of Review Times by Hour:**\n",
        "- The first plot shows the total number of reviews submitted at each hour of the day.\n",
        "- We can observe that there is a peak in reviews during certain hours, indicating when users are most active in leaving feedback. This could be related to business operating hours, user routines, or other factors.\n",
        "\n",
        "**Number of Unique Businesses Reviewed per Hour:**\n",
        "- The second plot shows the number of distinct businesses (identified by `gmap_id`) that received at least one review during each hour.\n",
        "- This helps understand if reviews are spread across many businesses throughout the day or concentrated on a few during specific times.\n",
        "\n",
        "Further analysis could involve looking at the average rating per hour, the types of businesses reviewed at different times, or correlating review times with other factors in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3**"
      ],
      "metadata": {
        "id": "QdkOlMSFENva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3.1**"
      ],
      "metadata": {
        "id": "UtIiytTrERdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rename the name column in the meta_data to avoid duplicate name error with the reviews data set\n",
        "df_meta = df_meta.withColumnRenamed(\"name\", \"business_name\")\n",
        "\n",
        "# Join the two DataFrames on the 'gmap_id' column\n",
        "joined_df = df_review.join(df_meta, on=\"gmap_id\", how=\"inner\")\n",
        "\n",
        "# Show the schema and some rows of the joined DataFrame\n",
        "print(\"Joined DataFrame Schema:\")\n",
        "joined_df.printSchema()\n",
        "\n",
        "print(\"Joined DataFrame (Top 5):\")\n",
        "joined_df.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "480yoGtO52Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the day of the week from the 'newtime' column\n",
        "joined_df_with_day = joined_df.withColumn(\"day_of_week\", date_format(col(\"newtime\"), \"E\"))\n",
        "\n",
        "# Group by day of the week and count the reviews\n",
        "reviews_by_day = joined_df_with_day.groupBy(\"day_of_week\").count()\n",
        "\n",
        "# Define the order of the days of the week for plotting\n",
        "day_order = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
        "\n",
        "# Convert to pandas DataFrame for plotting\n",
        "reviews_by_day_pd = reviews_by_day.toPandas()\n",
        "\n",
        "# Sort the pandas DataFrame by the defined day order\n",
        "reviews_by_day_pd['day_of_week'] = pd.Categorical(reviews_by_day_pd['day_of_week'], categories=day_order, ordered=True)\n",
        "reviews_by_day_pd = reviews_by_day_pd.sort_values('day_of_week')\n",
        "\n",
        "\n",
        "# Plot the results as a line chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=reviews_by_day_pd, x='day_of_week', y='count', marker='o')\n",
        "plt.title('Number of Reviews per Day of the Week (Joined Data)')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w54ao2_eCUEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Observations**\n",
        "\n",
        "The day of the week that has the highest reviews is Sunday, Friday has the lowest reviews."
      ],
      "metadata": {
        "id": "Gre5uj3qKl6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3.2**"
      ],
      "metadata": {
        "id": "9Ei1XwjWKUo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the day with the maximum number of reviews\n",
        "peak_day = reviews_by_day_pd.loc[reviews_by_day_pd['count'].idxmax()]['day_of_week']\n",
        "print(f\"The workday with the most reviews is: {peak_day}\")\n",
        "\n",
        "# Filter the joined DataFrame for the peak day\n",
        "peak_day_df = joined_df_with_day.filter(col(\"day_of_week\") == peak_day)\n",
        "\n",
        "# Group by business name and category and calculate the average rating\n",
        "avg_rating_by_business = peak_day_df.groupBy(\"business_name\", \"category\").agg(avg(\"rating\").alias(\"average_rating\"))\n",
        "\n",
        "# Find the business(es) with the highest average rating on the peak day\n",
        "highest_rated_businesses = avg_rating_by_business.orderBy(col(\"average_rating\").desc()).limit(5)\n",
        "\n",
        "print(f\"\\nBusinesses with the highest average rating on {peak_day} (Top 5):\")\n",
        "highest_rated_businesses.show(truncate=False)"
      ],
      "metadata": {
        "id": "Oliw0wxTDMIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3.3**"
      ],
      "metadata": {
        "id": "lyKfEILCEeiL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0549d5b"
      },
      "source": [
        "# Group by category and count the number of businesses\n",
        "category_counts = joined_df.groupBy(\"category\").count()\n",
        "\n",
        "# Order by count in descending order\n",
        "category_counts = category_counts.orderBy(col(\"count\").desc())\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "category_counts_pd = category_counts.toPandas()\n",
        "\n",
        "# Select the top 10 categories\n",
        "top_10_categories = category_counts_pd.head(10)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(data=top_10_categories, x='category', y='count', palette='viridis')\n",
        "plt.title('Top 10 Business Categories by Count')\n",
        "plt.xlabel('Business Category')\n",
        "plt.ylabel('Number of Businesses')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e45abe92"
      },
      "source": [
        "# Get the list of top 10 categories\n",
        "top_categories_list = top_10_categories['category'].tolist()\n",
        "\n",
        "# Filter the joined DataFrame to include only the top 10 categories\n",
        "filtered_df = joined_df_with_day.filter(col(\"category\").isin(top_categories_list))\n",
        "\n",
        "# Extract the hour of the day from the 'newtime' column\n",
        "filtered_df = filtered_df.withColumn(\"review_hour\", date_format(col(\"newtime\"), \"H\"))\n",
        "\n",
        "# Show the schema and some rows of the filtered DataFrame with the new column\n",
        "print(\"Filtered DataFrame Schema:\")\n",
        "filtered_df.printSchema()\n",
        "\n",
        "print(\"Filtered DataFrame (Top 5):\")\n",
        "filtered_df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ece5d5c"
      },
      "source": [
        "# Group by category and review_hour and count the number of reviews\n",
        "reviews_by_category_hour = filtered_df.groupBy(\"category\", \"review_hour\").count()\n",
        "\n",
        "# Find the peak review hour for each category\n",
        "\n",
        "window_spec = Window.partitionBy(\"category\").orderBy(col(\"count\").desc())\n",
        "\n",
        "ranked_reviews = reviews_by_category_hour.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "peak_hours_per_category = ranked_reviews.filter(col(\"rank\") == 1)\n",
        "\n",
        "# Display the peak review hour for each of the top 10 categories\n",
        "print(\"Peak review hour for each of the top 10 categories:\")\n",
        "peak_hours_per_category.orderBy(col(\"count\").desc()).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56150fc3"
      },
      "source": [
        "## Analyze average ratings by category\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb3b8c25"
      },
      "source": [
        "# Group by category and calculate the average rating\n",
        "avg_rating_by_category = joined_df.groupBy(\"category\").agg(avg(\"rating\").alias(\"average_rating\"))\n",
        "\n",
        "# Order by average_rating in descending order\n",
        "avg_rating_by_category = avg_rating_by_category.orderBy(col(\"average_rating\").desc())\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "avg_rating_by_category_pd = avg_rating_by_category.toPandas()\n",
        "\n",
        "# Select the top 10 categories by average rating\n",
        "top_10_avg_rating_categories = avg_rating_by_category_pd.head(10)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.barplot(data=top_10_avg_rating_categories, x='category', y='average_rating', palette='viridis')\n",
        "plt.title('Top 10 Business Categories by Average Rating')\n",
        "plt.xlabel('Business Category')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ac079f"
      },
      "source": [
        "**Analysis Summary:**\n",
        "\n",
        "Business Categories, Peak Hours, and Average Ratings\n",
        "This analysis explored review patterns based on business categories, focusing on the distribution of businesses, peak review hours for top categories, and average ratings across categories.\n",
        "\n",
        "**Distribution of Businesses Across Categories:**\n",
        "\n",
        "Based on the analysis of business categories, the most frequent categories in the dataset are:\n",
        "\n",
        "['Shopping mall']\n",
        "['Fast food restaurant', 'Breakfast restaurant', 'Hamburger restaurant', 'Restaurant', 'Sandwich shop']\n",
        "['Department store', 'Clothing store', 'Craft store', 'Shoe store', 'Sporting goods store']\n",
        "['Grocery store', 'Grocery delivery service']\n",
        "['Mexican restaurant']\n",
        "['Fast food restaurant', 'Breakfast restaurant', 'Burger joint', 'Restaurant', 'Sandwich shop']\n",
        "['Restaurant']\n",
        "['Warehouse store', 'Department store']\n",
        "['American restaurant']\n",
        "['Gas station']\n",
        "The bar plot titled \"Top 10 Business Categories by Count\" visually represents this distribution, clearly showing the categories with the highest number of associated businesses.\n",
        "\n",
        "**Peak Review Hours for Top Categories:**\n",
        "\n",
        "The analysis of peak review hours for the top 10 business categories revealed that for all of these categories, the hour with the highest number of reviews is '0' (which likely corresponds to midnight or the beginning of the day, depending on the exact time data representation).\n",
        "\n",
        "**Average Ratings by Category:**\n",
        "\n",
        "The calculation of average ratings by category identified the categories with the highest average ratings. The top 10 categories by average rating are:\n",
        "\n",
        "['Beauty salon', 'Hair care', 'Hair salon', 'Nail salon']\n",
        "['Snowboard shop', 'Clothing store', 'Shoe store', 'Ski shop', 'Sporting goods store', 'Store']\n",
        "['Horseback riding service', 'Bed & breakfast', 'Cabin rental agency', 'Cottage', 'Guest house']\n",
        "['Pediatric dentist', 'Dentist']\n",
        "['Pediatrician']\n",
        "['Surgeon', 'Doctor', 'Medical spa']\n",
        "['Waterfall', 'Tourist attraction']\n",
        "['Rafting', 'Raft trip outfitter', 'Scenic spot']\n",
        "['Dogsled ride service', 'Helicopter tour agency', 'Tour operator']\n",
        "['Leather goods store', 'Store']\n",
        "The bar plot titled \"Top 10 Business Categories by Average Rating\" illustrates these top-rated categories and their corresponding average ratings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all review text into a single string in PySpark\n",
        "all_reviews_text_df = joined_df.select(concat_ws(\" \", col(\"text\"))).collect()\n",
        "all_reviews_text = all_reviews_text_df[0][0] if all_reviews_text_df else \"\"\n",
        "\n",
        "\n",
        "# Remove punctuation and convert to lowercase using PySpark functions\n",
        "words_df = joined_df.select(lower(regexp_replace(col(\"text\"), r'[^\\w\\s]', '')).alias(\"text\"))\n",
        "words_df = words_df.select(explode(split(col(\"text\"), \"\\s+\")).alias(\"word\"))\n",
        "\n",
        "# Remove common English stop words using PySpark\n",
        "stop_words = set([\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
        "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"herself\",\n",
        "    \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\",\n",
        "    \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\",\n",
        "    \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\",\n",
        "    \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
        "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
        "    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
        "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
        "    \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
        "    \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
        "    \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"no\", \"review\"\n",
        "])\n",
        "\n",
        "words_df = words_df.filter(~col(\"word\").isin(stop_words)).filter(length(col(\"word\")) > 1)\n",
        "\n",
        "\n",
        "# Count the frequency of each word using PySpark\n",
        "word_counts_df = words_df.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
        "\n",
        "# Get the top 30 most common words\n",
        "top_30_words = word_counts_df.limit(30).collect()\n",
        "\n",
        "print(\"Top 30 most common words:\")\n",
        "for row in top_30_words:\n",
        "    print(f\"{row['word']}: {row['count']}\")\n",
        "\n",
        "# Generate word clouds for each year\n",
        "# Extract the year from the 'newtime' column\n",
        "reviews_with_year = joined_df.withColumn(\"review_year\", year(col(\"newtime\")))\n",
        "\n",
        "# Group by year and concatenate text for each year\n",
        "reviews_by_year_df = reviews_with_year.groupBy(\"review_year\").agg(concat_ws(\" \", collect_list(col(\"text\"))).alias(\"all_text\"))\n",
        "\n",
        "# Collect the data for word cloud generation\n",
        "reviews_by_year_pd = reviews_by_year_df.toPandas()"
      ],
      "metadata": {
        "id": "wUgBkQKWuWMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort reviews by year before plotting the word clouds\n",
        "reviews_by_year_pd.sort_values(by='review_year', inplace=True)\n",
        "reviews_by_year_pd.head()"
      ],
      "metadata": {
        "id": "3hMxZDzmoq8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate word cloud for each year\n",
        "num_years = len(reviews_by_year_pd)\n",
        "num_cols = 2\n",
        "num_rows = (num_years + num_cols - 1) // num_cols # Calculate number of rows needed\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10 * num_cols, 5 * num_rows))\n",
        "axes = axes.flatten() # Flatten the array of axes for easy iteration\n",
        "\n",
        "for i, (index, row) in enumerate(reviews_by_year_pd.iterrows()):\n",
        "    review_year_val = row['review_year']\n",
        "    text = row['all_text']\n",
        "\n",
        "    if text: # Check if there is any text for the year\n",
        "        # Remove punctuation and convert to lowercase\n",
        "        text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
        "        words = text.split()\n",
        "        words = [word for word in words if word not in stop_words and len(word) > 1]\n",
        "        text = \" \".join(words)\n",
        "\n",
        "        if text: # Check if there are any words left after removing stop words\n",
        "            wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "            axes[i].imshow(wordcloud, interpolation='bilinear')\n",
        "            axes[i].axis('off')\n",
        "            axes[i].set_title(f'Word Cloud for {review_year_val}')\n",
        "        else:\n",
        "            axes[i].axis('off') # Hide axis if no words to display\n",
        "            axes[i].set_title(f'No significant words for {review_year_val}')\n",
        "    else:\n",
        "        axes[i].axis('off') # Hide axis if no text for the year\n",
        "        axes[i].set_title(f'No reviews for {review_year_val}')\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uqufND1Fx74k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33b12f9d"
      },
      "source": [
        "### Insights from Top Words by Year\n",
        "\n",
        "Analyzing the top words for each year provides a clearer view of the review content and highlights shifts in focus over time:\n",
        "\n",
        "*   **Consistency of Core Themes:** Words like \"great,\" \"good,\" \"food,\" \"place,\" and \"service\" consistently appear in the top words across many years, indicating that these are fundamental aspects of the reviewed businesses that users frequently comment on.\n",
        "*   **Growth in Review Volume:** The increasing counts for the top words over the years (especially noticeable from 2016 onwards) directly reflect the significant growth in the number of reviews in the dataset during this period.\n",
        "*   **Dominance of Positive Language:** The prevalence of positive words like \"great,\" \"good,\" and \"amazing\" in the top lists suggests a generally positive sentiment within the reviews, based on the most frequent terms.\n",
        "*   **Specific Mentions:** While the very top words are quite general, looking beyond the top 5 or 10 for each year might reveal more specific keywords related to particular types of businesses or events that were prominent in those years.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.5**"
      ],
      "metadata": {
        "id": "Ah_-9i6IDq67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of unique reviewers per business\n",
        "unique_reviewers_per_business = joined_df.groupBy(\"business_name\", \"gmap_id\").agg(countDistinct(\"user_id\").alias(\"unique_reviewer_count\"))\n",
        "\n",
        "print(\"Number of unique reviewers per business (Top 10):\")\n",
        "unique_reviewers_per_business.orderBy(col(\"unique_reviewer_count\").desc()).show(10, truncate=False)\n",
        "\n",
        "# Determine the number of unique reviewers per category\n",
        "unique_reviewers_per_category = joined_df.groupBy(\"category\").agg(countDistinct(\"user_id\").alias(\"unique_reviewer_count\"))\n",
        "\n",
        "print(\"Number of unique reviewers per category (Top 10):\")\n",
        "unique_reviewers_per_category.orderBy(col(\"unique_reviewer_count\").desc()).show(10, truncate=False)\n",
        "\n",
        "# Analyze temporal patterns of reviews by year\n",
        "reviews_by_year = joined_df.withColumn(\"review_year\", year(col(\"newtime\"))).groupBy(\"review_year\").count().orderBy(\"review_year\")\n",
        "\n",
        "print(\"Number of reviews per year:\")\n",
        "reviews_by_year.show(truncate=False)\n",
        "\n",
        "# Analyze temporal patterns of reviews by month\n",
        "reviews_by_month = joined_df.withColumn(\"review_month\", month(col(\"newtime\"))).groupBy(\"review_month\").count().orderBy(\"review_month\")\n",
        "\n",
        "print(\"Number of reviews per month:\")\n",
        "reviews_by_month.show(truncate=False)\n",
        "\n",
        "# Analyze temporal patterns of reviews by day of the week\n",
        "reviews_by_dayofweek = joined_df.withColumn(\"review_dayofweek\", dayofweek(col(\"newtime\"))).groupBy(\"review_dayofweek\").count().orderBy(\"review_dayofweek\")\n",
        "\n",
        "print(\"Number of reviews per day of the week (1 = Sunday, 7 = Saturday):\")\n",
        "reviews_by_dayofweek.show(truncate=False)"
      ],
      "metadata": {
        "id": "X9W8FCkpyf-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights from Unique Reviewers and Temporal Patterns\n",
        "\n",
        "**Unique Reviewers per Business and Category:**\n",
        "\n",
        "*   The analysis of unique reviewers per business reveals the businesses that have attracted the largest number of distinct individuals to leave reviews. The top businesses with the highest unique reviewer counts are:\n",
        "    *   Moose's Tooth Pub & Pizzeria\n",
        "    *   Dimond Center\n",
        "    *   Walmart Supercenter (multiple locations appear in the top 10)\n",
        "    *   Costco Wholesale (multiple locations appear in the top 10)\n",
        "    *   Anchorage 5th Avenue Mall\n",
        "    *   49th State Brewing - Anchorage\n",
        "    *   Tikahtnu Commons\n",
        "\n",
        "*   Similarly, examining unique reviewers per category highlights the business categories that receive reviews from the broadest range of users. The top categories by unique reviewer count are:\n",
        "    *   ['Shopping mall']\n",
        "    *   ['Department store', 'Clothing store', 'Craft store', 'Discount store', 'Electronics store', 'Grocery store', 'Home goods store', 'Sporting goods store', 'Supermarket', 'Toy store']\n",
        "    *   ['Fast food restaurant', 'Breakfast restaurant', 'Coffee shop', 'Hamburger restaurant', 'Restaurant', 'Sandwich shop']\n",
        "    *   ['Grocery store', 'Grocery delivery service']\n",
        "    *   ['Mexican restaurant']\n",
        "    *   ['Warehouse store', 'Department store']\n",
        "    *   ['Fast food restaurant', 'Breakfast restaurant', 'Burrito restaurant', 'Lunch restaurant', 'Takeout Restaurant', 'Mexican restaurant', 'Restaurant', 'Taco restaurant', 'Tex-Mex restaurant', 'Vegetarian restaurant']\n",
        "    *   ['Restaurant']\n",
        "    *   ['American restaurant']\n",
        "    *   ['Grocery store', 'Propane supplier']\n",
        "\n",
        "These findings indicate that large retail centers, popular restaurants (especially fast food and casual dining), and general merchandise stores tend to attract reviews from a wider customer base.\n",
        "\n",
        "**Seasonal Patterns of Reviews:**\n",
        "\n",
        "*   **Reviews by Year:** The number of reviews has significantly increased over the years, with a substantial jump starting around 2016 and peaking in 2019 and 2020. This suggests a growing trend of users leaving reviews or an expansion of the dataset's coverage in recent years.\n",
        "\n",
        "* **Reviews by Month:** The months June to September have the highest review counts > 45k. This shows that most customers tend to spend more time during summer months shopping at large retail centers, restuarants and general merchandise shops. It might also be reflective customer behavioral patterns during different times of the year.\n",
        "\n",
        "* **Reviews by day of week** The days (saturday and sunday) have more review counts > 75k, compared to the rest of the days an indicator that customers tend to spend more times shopping during the weekend that over the rest of the days.\n"
      ],
      "metadata": {
        "id": "uMvJ65wBDlE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.6**"
      ],
      "metadata": {
        "id": "Jyv13UA09Paa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.6.1**"
      ],
      "metadata": {
        "id": "U5SdPuMk-x0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mark down comments Pending**"
      ],
      "metadata": {
        "id": "Tii81soO-0sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.6.2**"
      ],
      "metadata": {
        "id": "0NtvmG0C-uKx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af38ad06"
      },
      "source": [
        "# Create a user-item matrix using the relevant columns from the joined DataFrame\n",
        "# Select and cast the necessary columns\n",
        "ratings_df = joined_df.select(\n",
        "    col(\"user_id\").cast(IntegerType()).alias(\"userId\"),\n",
        "    col(\"gmap_id\").alias(\"itemId\"),\n",
        "    col(\"rating\").cast(DoubleType()).alias(\"rating\")\n",
        ")\n",
        "\n",
        "# Drop rows with null values in the selected columns\n",
        "ratings_df = ratings_df.dropna(subset=[\"userId\", \"itemId\", \"rating\"])\n",
        "\n",
        "# Convert item IDs to numerical IDs for ALS\n",
        "# Hash the string IDs\n",
        "ratings_df = ratings_df.withColumn(\"itemIdNumeric\", hash(col(\"itemId\")))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "(training, test) = ratings_df.randomSplit([0.8, 0.2], seed=123)\n",
        "\n",
        "# Build the recommendation model using ALS (Alternating Least Squares)\n",
        "# ALS is a common collaborative filtering algorithm suitable for sparse data\n",
        "als = ALS(maxIter=5, regParam=0.09, rank=20, userCol=\"userId\", itemCol=\"itemIdNumeric\", ratingCol=\"rating\",\n",
        "          coldStartStrategy=\"drop\")\n",
        "model = als.fit(training)\n",
        "\n",
        "# Evaluate the model by computing the RMSE on the test data\n",
        "predictions = model.transform(test)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "                                predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root-mean-square error = {rmse}\")\n",
        "\n",
        "# Generate top 10 business recommendations for each user\n",
        "userRecs = model.recommendForAllUsers(10)\n",
        "\n",
        "# Show the recommendations for a few users\n",
        "print(\"Top 10 recommendations for users:\")\n",
        "userRecs.show(5, truncate=False)\n",
        "\n",
        "# Generate top 10 user recommendations for each business\n",
        "itemRecs = model.recommendForAllItems(10)\n",
        "\n",
        "# Show the recommendations for a few businesses\n",
        "print(\"Top 10 user recommendations for businesses:\")\n",
        "itemRecs.show(5, truncate=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.7**"
      ],
      "metadata": {
        "id": "kSfKc7-f__gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.7.1**"
      ],
      "metadata": {
        "id": "vGWISAV8AGHL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44fae181"
      },
      "source": [
        "# Group by category and calculate the average rating\n",
        "avg_rating_by_category = joined_df.groupBy(\"category\").agg(avg(\"rating\").alias(\"average_rating\"))\n",
        "\n",
        "# Order by average_rating in descending order\n",
        "avg_rating_by_category = avg_rating_by_category.orderBy(col(\"average_rating\").desc())\n",
        "\n",
        "# Convert to pandas DataFrame for plotting\n",
        "avg_rating_by_category_pd = avg_rating_by_category.toPandas()\n",
        "\n",
        "# Select the top 20 categories by average rating for better visualization\n",
        "top_20_avg_rating_categories = avg_rating_by_category_pd.head(20)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.barplot(data=top_20_avg_rating_categories, x='category', y='average_rating', palette='viridis')\n",
        "plt.title('Top 20 Business Categories by Average Rating')\n",
        "plt.xlabel('Business Category')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the categories with the lowest average ratings\n",
        "bottom_20_avg_rating_categories = avg_rating_by_category_pd.tail(20)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.barplot(data=bottom_20_avg_rating_categories, x='category', y='average_rating', palette='viridis')\n",
        "plt.title('Bottom 20 Business Categories by Average Rating')\n",
        "plt.xlabel('Business Category')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z2CfekSh_tIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7afc34b1"
      },
      "source": [
        "### Insights from Relationship between Rating and Business Categories\n",
        "\n",
        "The visualizations above illustrate the relationship between business categories and their average ratings.\n",
        "\n",
        "**Top 20 Business Categories by Average Rating:**\n",
        "\n",
        "The first bar plot displays the top 20 business categories with the highest average ratings. Observing this plot, we can identify categories that consistently receive positive reviews from users. These might include niche services, specialized shops, or attractions that cater to specific interests and potentially exceed customer expectations within those areas. It's worth noting that some categories might have a smaller number of reviews contributing to their high average, which could make the average less representative.\n",
        "\n",
        "**Bottom 20 Business Categories by Average Rating:**\n",
        "\n",
        "The second bar plot shows the bottom 20 business categories with the lowest average ratings. This plot helps pinpoint categories that tend to receive lower ratings. These could be types of businesses that are more prone to customer complaints, have inherent challenges in meeting expectations, or face strong competition leading to critical reviews. Understanding these categories can be valuable for businesses within them to identify areas for improvement.\n",
        "\n",
        "**General Observations:**\n",
        "\n",
        "*   The plots reveal a variation in average ratings across different business categories.\n",
        "*   Some categories seem to consistently perform better in terms of customer satisfaction (based on average rating) than others.\n",
        "*   To gain deeper insights, it would be beneficial to also consider the number of reviews for each category when interpreting the average ratings, as categories with very few reviews might have skewed averages.\n",
        "*   Further analysis could involve examining the distribution of ratings within specific categories or exploring the common themes in the reviews for high- and low-rated categories."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.7.2**"
      ],
      "metadata": {
        "id": "e4y9yON4EDDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a threshold for low ratings (e.g., ratings less than or equal to 2)\n",
        "low_rating_threshold = 2\n",
        "\n",
        "# Filter the joined DataFrame to get reviews with low ratings\n",
        "low_rated_reviews = joined_df.filter(col(\"rating\") <= low_rating_threshold)\n",
        "\n",
        "# Count the number of low-rated reviews\n",
        "num_low_rated_reviews = low_rated_reviews.count()\n",
        "\n",
        "print(f\"Number of reviews with a rating of {low_rating_threshold} or less: {num_low_rated_reviews}\")\n",
        "\n",
        "# Extract the text from the low-rated reviews and combine it into a single string\n",
        "low_rated_text_df = low_rated_reviews.select(concat_ws(\" \", col(\"text\"))).collect()\n",
        "low_rated_text = low_rated_text_df[0][0] if low_rated_text_df else \"\"\n",
        "\n",
        "# Process the text: remove punctuation, convert to lowercase, and split into words\n",
        "if low_rated_text:\n",
        "    low_rated_words = low_rated_reviews.select(lower(regexp_replace(col(\"text\"), r'[^\\w\\s]', '')).alias(\"text\"))\n",
        "    low_rated_words = low_rated_words.select(explode(split(col(\"text\"), \"\\s+\")).alias(\"word\"))\n",
        "\n",
        "    # Remove common English stop words and words with length less than or equal to 1\n",
        "    stop_words_extended = stop_words.union({\"no\", \"review\"}) # Add \"no\" and \"review\" to stop words\n",
        "    low_rated_words = low_rated_words.filter(~col(\"word\").isin(stop_words_extended)).filter(length(col(\"word\")) > 1)\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    low_rated_word_counts_df = low_rated_words.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
        "\n",
        "    # Get the top 30 most common words in low-rated reviews\n",
        "    top_30_low_rated_words = low_rated_word_counts_df.limit(30).collect()\n",
        "\n",
        "    print(\"\\nTop 30 most common words in low-rated reviews:\")\n",
        "    for row in top_30_low_rated_words:\n",
        "        print(f\"{row['word']}: {row['count']}\")\n",
        "else:\n",
        "    print(\"\\nNo low-rated reviews found to analyze text.\")"
      ],
      "metadata": {
        "id": "kHfAxoveGKvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f4f3815"
      },
      "source": [
        "### **Analysis of Low-Rated Reviews**\n",
        "\n",
        "Based on the analysis of reviews with a rating of 2 or less:\n",
        "\n",
        "- There are **36884** reviews with a low rating.\n",
        "- The most common words appearing in these low-rated reviews include:\n",
        "\n",
        "\n",
        "**food**: This suggests a product indicating poor food quality or service\n",
        "\n",
        "**service**: This may relate to the quality of service, mostly poor services offered.\n",
        "\n",
        "**get**: This could relate to difficulty in receiving service or products\n",
        "\n",
        "**time**: This suggests that issues related to waiting times or the duration of service might be a common complaint.\n",
        "\n",
        "**like**: Can be used in various contexts, but in negative reviews, it might express disappointment or unfavorable comparisons.\n",
        "\n",
        "**bad**: A direct expression of dissatisfaction.\n",
        "\n",
        "**never**: Used to emphasize a consistently negative experience.\n",
        "\n",
        "**store**: Many low ratings are for retail establishments.\n",
        "\n",
        "**went**: Describes negative experiences encountered during a visit.\n",
        "\n",
        "**dont**: Indicates negative experiences or lack of something expected.\n",
        "\n",
        "These words collectively suggest that common reasons for low ratings include issues with service speed and efficiency, poor customer service and communication, problems with products or work quality, and negative experiences related to specific types of businesses like restaurants and retail stores."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.8**"
      ],
      "metadata": {
        "id": "a0DnpY96QgaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.8.1**"
      ],
      "metadata": {
        "id": "u_QVVynzUcVk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74e87f3e"
      },
      "source": [
        "# Sort reviews for each business by newtime\n",
        "window_spec_business = Window.partitionBy(\"gmap_id\").orderBy(asc(\"newtime\"))\n",
        "sorted_reviews_by_business = joined_df.withColumn(\"row_number\", rank().over(window_spec_business)).drop(\"row_number\")\n",
        "\n",
        "# Group by user_id and collect the list of business names\n",
        "user_business_list_df = sorted_reviews_by_business.groupBy(\"user_id\").agg(collect_list(\"business_name\").alias(\"business_list\"))\n",
        "\n",
        "# Convert the result to a list of (user_id, business_list) tuples (optional, for display)\n",
        "user_business_list = user_business_list_df.collect()\n",
        "\n",
        "# Display the result for a few users\n",
        "print(\"Business list for each user (first 20 users):\")\n",
        "for row in user_business_list[:20]:\n",
        "    print(f\"User ID: {row['user_id']}, Businesses: {row['business_list']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.8.2**"
      ],
      "metadata": {
        "id": "w35DT-eqWISM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check for repeated business names in a list\n",
        "def has_repeated_businesses(business_list):\n",
        "    if not business_list:\n",
        "        return False\n",
        "    # Convert the list to a set to find unique businesses\n",
        "    unique_businesses = set(business_list)\n",
        "    # If the number of unique businesses is less than the total number of reviews,\n",
        "    # it means there are repeated business names\n",
        "    return len(unique_businesses) < len(business_list)\n",
        "\n",
        "# Count the number of users with repeated business names in their list\n",
        "users_with_repeated_businesses_count = sum(\n",
        "    1 for user_id, business_list in user_business_list if has_repeated_businesses(business_list)\n",
        ")\n",
        "\n",
        "print(f\"Number of users with repeated business names in their review history: {users_with_repeated_businesses_count}\")"
      ],
      "metadata": {
        "id": "gKYBp2ciQi7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify and print duplicated business names and their counts for each reviewer\n",
        "print(\"\\nDuplicated business names and their counts for each user:\")\n",
        "count = 0\n",
        "for user_id, business_list in user_business_list:\n",
        "    if has_repeated_businesses(business_list):\n",
        "        # Use Counter to find duplicated business names and their counts\n",
        "        business_counts = Counter(business_list)\n",
        "        duplicated_businesses = {business: count for business, count in business_counts.items() if count > 1}\n",
        "        if duplicated_businesses:\n",
        "            print(f\"User ID: {user_id}\")\n",
        "            count += 1\n",
        "            for business, business_count in duplicated_businesses.items():\n",
        "                if count <= 1000:  # Limit to the first 1000 lines\n",
        "                    print(f\"  - {business}: {business_count} times\")\n",
        "                    count += 1\n",
        "                else:\n",
        "                    break # Exit the inner loop if 1000 lines are reached\n",
        "    if count > 1000: # Exit the outer loop if 1000 lines are reached\n",
        "        break"
      ],
      "metadata": {
        "id": "RFm7F0Q8lWRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "329ec989"
      },
      "source": [
        "# Create a new list to store user_id and a list of unique businesses\n",
        "user_business_list_unique = []\n",
        "\n",
        "# Iterate through the original list\n",
        "for user_id, business_list in user_business_list:\n",
        "    # Convert the business list to a set to get unique businesses, then convert back to a list\n",
        "    unique_businesses = list(set(business_list))\n",
        "    # Append the user_id and the list of unique businesses to the new list\n",
        "    user_business_list_unique.append((user_id, unique_businesses))\n",
        "\n",
        "# Display the result for a few users to verify\n",
        "print(\"Business list for each user with duplicates removed (first 20 users):\")\n",
        "for row in user_business_list_unique[:20]:\n",
        "    print(f\"User ID: {row[0]}, Businesses: {row[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.8.3**"
      ],
      "metadata": {
        "id": "bMRLoyi2WNM8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cb2574a"
      },
      "source": [
        "from pyspark.sql.functions import udf,expr\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Define a UDF to encode business names using hashing\n",
        "@udf(IntegerType())\n",
        "def encode_business_name(business_name):\n",
        "    if business_name is None:\n",
        "        return None\n",
        "    return hash(business_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "898acacc"
      },
      "source": [
        "# Apply the hash function directly to each element in the business_list column\n",
        "encoded_user_business_list_df = user_business_list_df.withColumn(\n",
        "    \"encoded_business_list\",\n",
        "    # Use transform to apply the hash function to each element in the list\n",
        "    expr(\"transform(business_list, business -> hash(business))\")\n",
        ")\n",
        "\n",
        "# Display the result for a few users to verify\n",
        "print(\"Business list for each user with encoded business names (first 20 users):\")\n",
        "for row in encoded_user_business_list_df.collect()[:20]:\n",
        "    print(f\"User ID: {row['user_id']}, Encoded Businesses: {row['encoded_business_list']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_user_business_list_df.columns"
      ],
      "metadata": {
        "id": "cOx-fVtEqXEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "# NOTE: This assumes 'df' is the pandas DataFrame created in the notebook (Cell 15)\n",
        "\n",
        "# Group the DataFrame by user_id and collect the unique gmap_id's into a set for fast lookups\n",
        "encoded_user_business_list_series = df.groupby('user_id')['gmap_id'].apply(lambda x: set(x.unique()))\n",
        "\n",
        "# Convert the Series back to a DataFrame with a descriptive column name\n",
        "encoded_user_business_list_df = encoded_user_business_list_series.rename('reviewed_businesses').to_frame()\n",
        "\n",
        "print(\"--- Encoded User-Business List (Sample) ---\")\n",
        "display(encoded_user_business_list_df.head())"
      ],
      "metadata": {
        "id": "NNfxxBOSsgvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- Jaccard Similarity Function (Unchanged, it is efficient) ---\n",
        "def calculate_jaccard_similarity(set_a, set_b):\n",
        "    \"\"\"\n",
        "    Calculates the Jaccard similarity between two sets.\n",
        "    \"\"\"\n",
        "    # Calculate intersection and union size\n",
        "    intersection = len(set_a.intersection(set_b))\n",
        "    union = len(set_a.union(set_b))\n",
        "\n",
        "    if union == 0:\n",
        "        return 0.0\n",
        "    return intersection / union\n",
        "\n",
        "# --- Calculate Pairwise Similarities (Refactored for Sparsity) ---\n",
        "\n",
        "# Get the list of all unique users and their business sets\n",
        "# NOTE: user_sets must be a Series/Dictionary mapping user_id to a set of business IDs.\n",
        "# For example: user_sets = encoded_user_business_list_series\n",
        "user_sets = encoded_user_business_list_series # Assuming this is available\n",
        "user_ids = user_sets.index.tolist()\n",
        "\n",
        "# ⚠️ MEMORY SAVING CHANGE: Use a list to store only non-zero similarities\n",
        "# This avoids allocating space for all N*N zero-similarity pairs.\n",
        "sparse_similarity_data = []\n",
        "\n",
        "# Iterate through all unique pairs of users\n",
        "for user_a, user_b in combinations(user_ids, 2):\n",
        "    set_a = user_sets.loc[user_a]\n",
        "    set_b = user_sets.loc[user_b]\n",
        "\n",
        "    similarity = calculate_jaccard_similarity(set_a, set_b)\n",
        "\n",
        "    # ⚠️ CRITICAL STEP: Store only pairs with positive similarity (they share at least one business)\n",
        "    if similarity > 0:\n",
        "        # Store symmetrically: (A, B, sim) and (B, A, sim)\n",
        "        sparse_similarity_data.append({'user_a': user_a, 'user_b': user_b, 'similarity': similarity})\n",
        "        sparse_similarity_data.append({'user_a': user_b, 'user_b': user_a, 'similarity': similarity})\n",
        "\n",
        "# Convert the list of sparse results into a Pandas DataFrame\n",
        "sparse_similarity_df = pd.DataFrame(sparse_similarity_data)\n",
        "\n",
        "# Add self-similarity (1.0) for every user\n",
        "# This is required if the system needs to recommend items from the user's own history\n",
        "self_similarity_data = [{'user_a': uid, 'user_b': uid, 'similarity': 1.0} for uid in user_ids]\n",
        "sparse_similarity_df = pd.concat([sparse_similarity_df, pd.DataFrame(self_similarity_data)], ignore_index=True)\n",
        "\n",
        "print(\"\\n--- Sparse User Similarity Data (Sample) ---\")\n",
        "display(sparse_similarity_df.head())"
      ],
      "metadata": {
        "id": "wPcRkzo9ElFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jaccard similarity is a statistic used for comparing the similarity and diversity of sample sets. In this context, we're using it to compare the sets of businesses reviewed by different customers.\n",
        "\n",
        "The strategy used is as follows:\n",
        "\n",
        "Representing Businesses as Sets: For each user, we create a set of the unique businesses they have reviewed using the encoded business names.\n",
        "\n",
        "Calculating Intersection: The intersection of two users' sets includes the businesses that both users have reviewed.\n",
        "\n",
        "Calculating Union: The union of two users' sets includes all unique businesses that either user has reviewed.\n",
        "\n",
        "Jaccard Similarity Formula: The Jaccard similarity is calculated as the size of the intersection divided by the size of the union:\n",
        "\n",
        "Jaccard Similarity (User A, User B) = |Set of businesses reviewed by A ∩ Set of businesses reviewed by B| / |Set of businesses reviewed by A ∪ Set of businesses reviewed by B|\n",
        "\n",
        "A Jaccard similarity of 1 means the two users have reviewed the exact same set of businesses, while a similarity of 0 means they have reviewed completely different sets of businesses. Values in between indicate varying degrees of overlap in their review history.\n",
        "\n",
        "In the code, we are calculating this similarity based on the TF-IDF vectors derived from the encoded business names. The calculate_jaccard_similarity function does this by looking at the indices of the non-zero elements in the sparse vectors, which correspond to the businesses present in the user's review history."
      ],
      "metadata": {
        "id": "vzBSo8ZqiM8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 2**"
      ],
      "metadata": {
        "id": "7Ca7r1oW6oVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0 — Packages: install and import (simple and annotated)\n",
        "# Why: set up a small, standard toolbox for time series work:\n",
        "# - numpy/pandas/matplotlib/seaborn for data and plots\n",
        "# - statsmodels for classical TS models and diagnostics\n",
        "# - scikit-learn for metrics and simple baselines\n",
        "# - prophet for later structural forecasting\n",
        "# Notes: keep imports lean and human-readable.\n",
        "\n",
        "# Make sure any old \"fbprophet\" is removed and Prophet is available\n",
        "%pip -q install prophet\n",
        "\n",
        "# Core scientific stack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Classical time series (statsmodels)\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# ML utilities\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Prophet (structural forecasting)\n",
        "from prophet import Prophet\n",
        "\n",
        "# Spark helpers (for aggregation in the next cell)\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Plot style\n",
        "%matplotlib inline\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# References:\n",
        "# Hunter, J.D. (2007) Matplotlib: A 2D graphics environment. Computing in Science & Engineering, 9(3), 90–95.\n",
        "# McKinney, W. (2010) Data structures for statistical computing in Python. In: Proceedings of the 9th Python in Science Conference, pp. 51–56.\n",
        "# Waskom, M. (2021) seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021.\n",
        "# Seabold, S. and Perktold, J. (2010) statsmodels: Econometric and statistical modeling with Python. Proceedings of the 9th Python in Science Conference.\n",
        "# Pedregosa, F. et al. (2011) Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830.\n",
        "# Taylor, S.J. and Letham, B. (2018) Forecasting at scale. The American Statistician, 72(1), 37–45."
      ],
      "metadata": {
        "id": "VwSA1U3L7FTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 — Select dataframe and prepare it for analysis\n",
        "# Task: from joined_df, compute daily review counts and make a clean pandas time series.\n",
        "# Why: most TS methods expect a regular date index with one value per day.\n",
        "# Method: group by date (newtime), count records, convert to pandas, set DateTimeIndex.\n",
        "# Cite: aggregation of counts to a regular TS is standard in classical workflows (Box et al., 2015).\n",
        "\n",
        "# Safety: check joined_df exists\n",
        "try:\n",
        "    _ = joined_df.printSchema()\n",
        "except NameError:\n",
        "    raise RuntimeError(\"joined_df not found. Please run the earlier join cell to create it.\")\n",
        "\n",
        "# 1) Aggregate to reviews per day in Spark\n",
        "daily_counts_spark = (\n",
        "    joined_df\n",
        "    .where(F.col(\"newtime\").isNotNull())\n",
        "    .groupBy(\"newtime\")\n",
        "    .agg(F.count(F.lit(1)).alias(\"reviews\"))\n",
        "    .orderBy(\"newtime\")\n",
        ")\n",
        "\n",
        "# 2) Move to pandas and set a clean daily index\n",
        "daily_pdf = daily_counts_spark.toPandas()\n",
        "daily_pdf[\"newtime\"] = pd.to_datetime(daily_pdf[\"newtime\"])\n",
        "daily_pdf = daily_pdf.sort_values(\"newtime\").set_index(\"newtime\")\n",
        "\n",
        "print(\"First 5 observed days (before any filling):\")\n",
        "display(daily_pdf.head())\n",
        "\n",
        "print(\"\\nQuick shape and date span:\")\n",
        "print(\"rows:\", len(daily_pdf), \"| cols:\", list(daily_pdf.columns))\n",
        "print(\"range:\", daily_pdf.index.min().date(), \"to\", daily_pdf.index.max().date())\n",
        "\n",
        "# References (Harvard style):\n",
        "# Box, G.E.P., Jenkins, G.M., Reinsel, G.C. and Ljung, G.M. (2015) Time Series Analysis: Forecasting and Control. 5th edn. Hoboken, NJ: Wiley."
      ],
      "metadata": {
        "id": "_wca_mmD9AEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 2.1**"
      ],
      "metadata": {
        "id": "pCT7QOgH6rXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Standardize the date column (convert to datetime and normalize to date only)\n",
        "df['review_date'] = pd.to_datetime(df['review_date']).dt.normalize()\n",
        "\n",
        "# Create the daily time series by counting reviews per day\n",
        "daily_reviews = df.groupby('review_date').size().rename('review_count')\n",
        "\n",
        "# Determine the full date range of the dataset\n",
        "min_date = daily_reviews.index.min()\n",
        "max_date = daily_reviews.index.max()\n",
        "full_date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
        "\n",
        "# Reindex the time series to include all days in the range (this introduces NaNs for missing days)\n",
        "full_ts = daily_reviews.reindex(full_date_range)\n",
        "\n",
        "# Calculate the mean review count for imputation: Total Reviews / Total Days in Range\n",
        "total_reviews = daily_reviews.sum()\n",
        "total_days = len(full_date_range)\n",
        "mean_reviews_imputation = total_reviews / total_days\n",
        "\n",
        "# Fill the missing days (NaNs) with the calculated mean\n",
        "imputed_ts = full_ts.fillna(mean_reviews_imputation)\n",
        "imputed_ts.index.freq = 'D' # Set frequency for decomposition\n",
        "\n",
        "print(\"--- Imputation Details ---\")\n",
        "print(f\"Total Reviews: {total_reviews}\")\n",
        "print(f\"Total Days in Full Range: {total_days}\")\n",
        "print(f\"Calculated Mean Reviews per Day (Imputation Value): {mean_reviews_imputation:.2f}\")\n",
        "print(f\"Number of Missing Days Filled: {len(daily_reviews) - len(imputed_ts.dropna())}\")\n",
        "\n",
        "# Decompose the time series using the Additive Model with a weekly period (7 days)\n",
        "decomposition = seasonal_decompose(imputed_ts, model='additive', period=7)\n",
        "\n",
        "#Analysis and Plotting ---\n",
        "fig, axes = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n",
        "decomposition.observed.plot(ax=axes[0], title='Observed Review Submissions (Daily)')\n",
        "decomposition.trend.plot(ax=axes[1], title='Trend Component')\n",
        "decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component (Weekly)')\n",
        "decomposition.resid.plot(ax=axes[3], title='Residual Component')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XJlv9sK658Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 2.2**"
      ],
      "metadata": {
        "id": "1K0CuL1MBHzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the Time Series (80% Train, 20% Test)\n",
        "train_size = int(len(imputed_ts) * 0.8)\n",
        "train, test = imputed_ts[:train_size], imputed_ts[train_size:]\n",
        "print(f\"Training set size: {len(train)}\")\n",
        "print(f\"Testing set size: {len(test)}\")\n",
        "\n",
        "#Define the Grid Search Parameters\n",
        "p_params = d_params = q_params = [0, 1, 2]\n",
        "pdq_combinations = list(product(p_params, d_params, q_params))\n",
        "\n",
        "best_mae = float('inf')\n",
        "best_order = None\n",
        "results = []\n",
        "\n",
        "print(\"\\n--- Starting ARIMA Grid Search (p, d, q in [0, 1, 2]) ---\")\n",
        "\n",
        "#Grid Search Loop\n",
        "for order in pdq_combinations:\n",
        "    try:\n",
        "        # Train the ARIMA Model\n",
        "        model = ARIMA(train, order=order)\n",
        "        model_fit = model.fit()\n",
        "\n",
        "        # Forecast\n",
        "        # Start and end indices for the forecast period (should match the test set)\n",
        "        start_index = len(train)\n",
        "        end_index = len(imputed_ts) - 1\n",
        "        forecast = model_fit.predict(start=start_index, end=end_index)\n",
        "\n",
        "        # Calculate MAE\n",
        "        mae = mean_absolute_error(test, forecast)\n",
        "        results.append({'order': order, 'mae': mae})\n",
        "\n",
        "        # Update Best Model\n",
        "        if mae < best_mae:\n",
        "            best_mae = mae\n",
        "            best_order = order\n",
        "            # Print the new best model as it's found\n",
        "            print(f\"-> New Best: ARIMA{order} - MAE: {mae:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Skip models that fail to converge or raise other errors\n",
        "        continue\n",
        "\n",
        "# Final Output\n",
        "results_df = pd.DataFrame(results).sort_values(by='mae')\n",
        "\n",
        "print(\"\\n--- Grid Search Complete ---\")\n",
        "print(f\"Total models tested: {len(results)}\")\n",
        "print(f\"Best ARIMA Model: ARIMA{best_order} with MAE: {best_mae:.4f}\")\n",
        "print(\"\\nTop 5 Models by MAE:\")\n",
        "display(results_df.head())"
      ],
      "metadata": {
        "id": "jCcX-0Bd6hRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HhM3c1N9BVbN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
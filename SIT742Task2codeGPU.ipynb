{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnmuturi/SIT742/blob/main/SIT742Task2codeGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**END TERM ASSIGNMENT**\n",
        "\n",
        " **GROUP C30 Members**\n",
        "\n",
        "1. David Muturi - S225177509\n",
        "2. Nhlanhla Matukane - S225177376\n",
        "3. Vincent Nwobi - S225177483"
      ],
      "metadata": {
        "id": "SzkIZK2xplg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install RAPIDS for Google Colab with A100\n",
        "# First check your environment\n",
        "!nvidia-smi\n",
        "!nvcc --version\n",
        "\n",
        "# Install conda (required for RAPIDS on Colab)\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import condacolab\n",
        "except:\n",
        "    !pip install -q condacolab\n",
        "    import condacolab\n",
        "\n",
        "# Install conda\n",
        "condacolab.install()\n",
        "\n",
        "# Install RAPIDS using conda (more reliable for Colab)\n",
        "!conda install -c rapidsai -c conda-forge -c nvidia rapids=24.08 python=3.10 cuda-version=12.4 -y\n",
        "\n",
        "# Alternative: Try pip with specific versions\n",
        "# !pip install cupy-cuda12x\n",
        "# !pip install --extra-index-url=https://pypi.nvidia.com cudf-cu12==24.08.3 dask-cudf-cu12==24.08.3\n",
        "# !pip install --extra-index-url=https://pypi.nvidia.com cugraph-cu12==24.08.3 pylibcugraph-cu12==24.08.3\n",
        "\n",
        "# Restart runtime might be needed\n",
        "# import os\n",
        "# os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBasoKwF28oF",
        "outputId": "20b420ed-30dd-40eb-800e-900b75a1d8d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct  4 22:04:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "‚ú®üç∞‚ú® Everything looks OK!\n",
            "Channels:\n",
            " - rapidsai\n",
            " - conda-forge\n",
            " - nvidia\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\bfailed\n",
            "\n",
            "SpecsConfigurationConflictError: Requested specs conflict with configured specs.\n",
            "  requested specs: \n",
            "    - cuda-version=12.4\n",
            "    - python=3.10\n",
            "    - rapids=24.08\n",
            "  pinned specs: \n",
            "    - cuda-version=12\n",
            "    - python=3.12\n",
            "    - python_abi=3.12[build=*cp312*]\n",
            "Use 'conda config --show-sources' to look for 'pinned_specs' and 'track_features'\n",
            "configuration parameters.  Pinned specs may also be defined in the file\n",
            "/usr/local/conda-meta/pinned.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 1**"
      ],
      "metadata": {
        "id": "cdkp7fJMbMMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.1**"
      ],
      "metadata": {
        "id": "G0MnsqpJbQ5r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53955062",
        "outputId": "41256e7a-3786-4f04-b3e6-ec6d517d125b"
      },
      "source": [
        "#install the spark library and other dependencies\n",
        "!pip install pyspark\n",
        "!apt-get install pandoc\n",
        "!pip install requests wget\n",
        "\n",
        "# Remove the RAPIDS installation and imports as they are not needed for the rest of the notebook\n",
        "# !pip -q install -U \\\n",
        "#     cudf-cu12 cugraph-cu12 rmm-cu12 cupy-cuda12x \\\n",
        "#     --extra-index-url=https://pypi.nvidia.com\n",
        "\n",
        "# import cudf, cugraph, cupy\n",
        "# print(\"cuDF version:\", cudf.__version__)\n",
        "# print(\"cuGraph version:\", cugraph.__version__)\n",
        "# print(\"CuPy version:\", cupy.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/site-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.11/site-packages (from pyspark) (0.10.9.9)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pandoc is already the newest version (2.9.2.1-3ubuntu2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (2.32.3)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.11/site-packages (3.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load necessary libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from google.colab import drive\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, from_unixtime, to_date, when, lit, trim, count, date_format,avg,struct, asc\n",
        "from pyspark.sql.functions import lower, regexp_replace, split, explode, year, concat_ws, length, collect_list,rank,countDistinct, month, dayofweek\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import hash\n",
        "import re\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from itertools import product\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "import wget\n",
        "import zipfile\n",
        "import os"
      ],
      "metadata": {
        "id": "C4Ktnb9OqiWo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "9d5f5afd-359d-4188-ede4-41f12fbbcbff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'wget'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-48018754.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wget'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_url = 'https://github.com/tulip-lab/sit742/raw/refs/heads/develop/Jupyter/data/business_review_submission.zip'\n",
        "# The desired filename for the downloaded zip file\n",
        "zip_filename = 'business_review_submission.zip'\n",
        "# The directory to extract the contents into\n",
        "extract_directory = '/content/sample_data'\n",
        "\n",
        "\n",
        "# --- Download the zip file ---\n",
        "print(f\"Downloading file from {zip_url}...\")\n",
        "try:\n",
        "    wget.download(zip_url, zip_filename)\n",
        "    print(f\"\\nDownload complete. File saved as '{zip_filename}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during download: {e}\")\n",
        "    # Exit if download fails\n",
        "    exit()\n",
        "\n",
        "# --- Access the contents ---\n",
        "# Ensure the extraction directory exists\n",
        "os.makedirs(extract_directory, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    # Open the zip file for reading\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        print(f\"Extracting contents to '{extract_directory}'...\")\n",
        "        # Extract all files from the zip archive\n",
        "        zip_ref.extractall(extract_directory)\n",
        "        print(\"Extraction complete.\")\n",
        "\n",
        "        # Optional: List and read a file from the extracted contents\n",
        "        print(\"\\nContents of extracted files:\")\n",
        "        for file_name in zip_ref.namelist():\n",
        "            print(f\"- {file_name}\")\n",
        "            # Example: Read and print the content of a specific file\n",
        "            if file_name.endswith('.txt'):\n",
        "                with zip_ref.open(file_name) as file_content:\n",
        "                    print(file_content.read().decode('utf-8'))\n",
        "\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: The downloaded file '{zip_filename}' is not a valid zip file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during extraction: {e}\")"
      ],
      "metadata": {
        "id": "laoPpoq6GGbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the directory where files were extracted\n",
        "extraction_directory = '/content/sample_data/'\n",
        "\n",
        "# Update the paths to include the extraction directory\n",
        "csv_path = os.path.join(extraction_directory, 'review.csv')\n",
        "meta_csv_path = os.path.join(extraction_directory, 'meta-review-business.csv')\n",
        "\n",
        "# Now you can load the CSV files using the correct paths\n",
        "df_reviews = pd.read_csv(csv_path)\n",
        "df_meta = pd.read_csv(meta_csv_path)\n",
        "\n",
        "print(\"\\nFiles successfully loaded from the extraction directory!\")\n",
        "\n",
        "# Display the first few rows of each DataFrame\n",
        "print(\"\\n--- Reviews DataFrame ---\")\n",
        "display(df_reviews.head())\n",
        "\n",
        "print(\"\\n--- Meta DataFrame ---\")\n",
        "display(df_meta.head())"
      ],
      "metadata": {
        "id": "6hX6tACBLWy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReviewAnalysis\").getOrCreate()\n",
        "\n",
        "# --- Load review.csv ---\n",
        "print(\"Loading 'review.csv' into a Spark DataFrame...\")\n",
        "try:\n",
        "    # Use the path from your file upload cell: 'review.csv'\n",
        "    df_review = spark.read.csv(\n",
        "        csv_path,\n",
        "        header=True,\n",
        "        inferSchema=True,\n",
        "        sep=',',\n",
        "        quote='\"',\n",
        "        escape='\"',\n",
        "        multiLine=True\n",
        "    )\n",
        "    print(\"review.csv' loaded successfully!\")\n",
        "    df_review.show(5, truncate=False)  # Display the first five rows without truncating\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading 'review.csv': {e}\")\n",
        "\n",
        "\n",
        "# --- Load meta-review-business.csv ---\n",
        "print(\"\\nLoading 'meta-review-business.csv' into a Spark DataFrame...\")\n",
        "try:\n",
        "    # Use the path from your file upload cell: 'meta-review-business.csv'\n",
        "    df_meta= spark.read.csv(\n",
        "        meta_csv_path,\n",
        "        header=True,\n",
        "        inferSchema=True,\n",
        "        sep=',',\n",
        "        quote='\"',\n",
        "        escape='\"',\n",
        "        multiLine=True\n",
        "    )\n",
        "    print(\"meta-review-business.csv' loaded successfully!\")\n",
        "    df_meta.show(5, truncate=False)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading 'meta-review-business.csv': {e}\")"
      ],
      "metadata": {
        "id": "fTGb3fMze0TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation for df_review\n",
        "print(\"--- DataFrame: df_review ---\")\n",
        "total_reviews = df_review.count()\n",
        "print(f\"Total rows: {total_reviews:,}\")\n",
        "print(\"\\nSchema:\")\n",
        "df_review.printSchema()"
      ],
      "metadata": {
        "id": "O0c_ydRPs_3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show summary statistics\n",
        "df_review.describe().show()"
      ],
      "metadata": {
        "id": "X0bNcdeKJ3Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation for df_meta\n",
        "print(\"\\n\\n DataFrame: df_meta\")\n",
        "total_businesses = df_meta.count()\n",
        "print(f\"Total rows: {total_businesses:,}\")\n",
        "print(\"\\nSchema:\")\n",
        "df_meta.printSchema()"
      ],
      "metadata": {
        "id": "-XavS7cCtJos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation for df_review\n",
        "print(\"--- DataFrame: df_review ---\")\n",
        "total_reviews = df_review.count()\n",
        "print(f\"Total rows: {total_reviews:,}\")\n",
        "print(\"\\nSchema:\")\n",
        "df_review.printSchema()"
      ],
      "metadata": {
        "id": "XshvR9_ms3hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Statistics for df_meta ---\n",
        "print(\"\\n Summary Statistics for df_meta\")\n",
        "df_meta.describe().show()"
      ],
      "metadata": {
        "id": "34PcdbfFtPbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before any further analysis, we evaluated the missing values in the dataset and used the Spark DataFrame functions to count them. The normal way to deal with missing values is to treat nulls as missing, empty strings as missing for text, and NaN as missing for numbers (Little and Rubin, 2019; van Buuren, 2018).\n",
        "# We do this with Apache Spark SQL functions and data types. The meanings of isNull, isnan, and trim are in the Spark docs.These are consistent with SQL-style null handling and are in line with how SQL handles nulls (Apache Spark, 2025).\n",
        "\n",
        "from pyspark.sql.functions import col, trim, when, isnan, lit, sum as F_sum, count as F_count\n",
        "from pyspark.sql.types import StringType, NumericType\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def missing_profile_spark(df, df_name=\"dataframe\"):\n",
        "    \"\"\"\n",
        "    Build a tidy table with missingness and completeness per column for a Spark DataFrame.\n",
        "    Returns a pandas.DataFrame for easy viewing and saving.\n",
        "    \"\"\"\n",
        "    # Basic checks\n",
        "    total_rows = df.count()\n",
        "    if total_rows == 0:\n",
        "        print(f\"[{df_name}] has 0 rows. Nothing to profile.\")\n",
        "        return pd.DataFrame(columns=[\"column\", \"dtype\", \"missing_count\", \"missing_rate\",\n",
        "                                     \"non_missing_count\", \"completeness_rate\"])\n",
        "\n",
        "    # Build \"missing flag\" per column in one pass to be efficient\n",
        "    cols = []\n",
        "    dtypes = []\n",
        "    missing_flags = []\n",
        "\n",
        "    for f in df.schema.fields:\n",
        "        c = f.name\n",
        "        dt = f.dataType\n",
        "        cols.append(c)\n",
        "        dtypes.append(str(dt))\n",
        "\n",
        "        # Define what counts as missing for this column\n",
        "        is_null = col(c).isNull()\n",
        "        if isinstance(dt, StringType):\n",
        "            is_blank = (trim(col(c)) == \"\")\n",
        "            flag = when(is_null | is_blank, 1).otherwise(0)\n",
        "        elif isinstance(dt, NumericType):\n",
        "            flag = when(is_null | isnan(col(c)), 1).otherwise(0)\n",
        "        else:\n",
        "            # For non-string, non-numeric types, we count only null as missing\n",
        "            flag = when(is_null, 1).otherwise(0)\n",
        "\n",
        "        missing_flags.append(F_sum(flag).alias(c))\n",
        "\n",
        "    # Aggregate all missing counts in one go\n",
        "    counts_row = df.agg(*missing_flags).collect()[0].asDict()\n",
        "\n",
        "    # Build a tidy pandas table\n",
        "    rows = []\n",
        "    for c, dt in zip(cols, dtypes):\n",
        "        miss = int(counts_row.get(c, 0))\n",
        "        miss_rate = miss / float(total_rows)\n",
        "        non_miss = total_rows - miss\n",
        "        comp_rate = 1.0 - miss_rate\n",
        "        rows.append({\n",
        "            \"column\": c,\n",
        "            \"dtype\": dt,\n",
        "            \"missing_count\": miss,\n",
        "            \"missing_rate\": round(miss_rate, 6),\n",
        "            \"non_missing_count\": non_miss,\n",
        "            \"completeness_rate\": round(comp_rate, 6)\n",
        "        })\n",
        "\n",
        "    out = pd.DataFrame(rows).sort_values([\"missing_rate\", \"missing_count\", \"column\"], ascending=[False, False, True]).reset_index(drop=True)\n",
        "\n",
        "    # Print a compact summary and return the full table\n",
        "    print(f\"\\n[{df_name}] rows: {total_rows}, columns: {len(cols)}\")\n",
        "    print(\"Top columns by missing_rate:\")\n",
        "    display(out.head(10))\n",
        "    return out\n",
        "\n",
        "# Run profiles for both data sets\n",
        "review_profile = missing_profile_spark(df_review, df_name=\"df_review\")\n",
        "meta_profile   = missing_profile_spark(df_meta,   df_name=\"df_meta\")\n",
        "\n",
        "\n",
        "# References (Harvard style):\n",
        "# Apache Spark (2025) Spark SQL, Built-in Functions. Available at: https://spark.apache.org/docs/latest/api/python/ (Accessed: 03/10/2025).\n",
        "# Little, R.J.A. and Rubin, D.B. (2019) Statistical Analysis with Missing Data. 3rd edn. Hoboken, NJ: Wiley.\n",
        "# van Buuren, S. (2018) Flexible Imputation of Missing Data. 2nd edn. Boca Raton, FL: CRC Press. Available at: https://stefvanbuuren.name/fimd/ (Accessed: 03/10/2025)."
      ],
      "metadata": {
        "id": "JSohcDMQvZmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_review ‚Äî missingness bar plot\n",
        "# Idea:\n",
        "# - Convert a small slice from Spark to pandas (for plotting).\n",
        "# - Treat empty strings \"\" as missing by converting them to NaN.\n",
        "# - Plot a single missingness bar chart to see gaps per column.\n",
        "\n",
        "!pip -q install missingno\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "\n",
        "# Convert up to N rows to pandas\n",
        "N =  521515  # number of observations\n",
        "pdf_review = df_review.limit(N).toPandas()\n",
        "\n",
        "# Empty strings -> NaN so they count as missing for text columns\n",
        "pdf_review = pdf_review.replace(r\"^\\s*$\", np.nan, regex=True)\n",
        "\n",
        "# One clear plot\n",
        "plt.figure()\n",
        "msno.bar(pdf_review)\n",
        "plt.title(\"df_review - missingness by column (bar)\")\n",
        "plt.show()\n",
        "\n",
        "# Quick text summary (top 10 by missing rate)\n",
        "print(\"\\nTop 10 columns by missing rate (sampled):\")\n",
        "print(pdf_review.isna().mean().sort_values(ascending=False).head(10).round(3))\n"
      ],
      "metadata": {
        "id": "I5aP3oPB30X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_meta ‚Äî missingness bar plot\n",
        "# Same approach as df_review to keep things consistent.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "\n",
        "# Convert up to N rows to pandas\n",
        "N = 12774 # number of observations\n",
        "pdf_meta = df_meta.limit(N).toPandas()\n",
        "\n",
        "# Empty strings -> NaN\n",
        "pdf_meta = pdf_meta.replace(r\"^\\s*$\", np.nan, regex=True)\n",
        "\n",
        "# One clear plot\n",
        "plt.figure()\n",
        "msno.bar(pdf_meta)\n",
        "plt.title(\"df_meta - missingness by column (bar)\")\n",
        "plt.show()\n",
        "\n",
        "# Quick text summary (top 10 by missing rate)\n",
        "print(\"\\nTop 10 columns by missing rate (sampled):\")\n",
        "print(pdf_meta.isna().mean().sort_values(ascending=False).head(10).round(3))\n",
        "\n",
        "# References (Harvard style):\n",
        "# Bilogur, A. (2018) missingno: a missing data visualization suite. Available at: https://github.com/ResidentMario/missingno (Accessed: 04/10/2025).\n",
        "# Little, R.J.A. and Rubin, D.B. (2019) Statistical Analysis with Missing Data. 3rd edn. Hoboken, NJ: Wiley.\n",
        "# van Buuren, S. (2018) Flexible Imputation of Missing Data. 2nd edn. Boca Raton, FL: CRC Press. Available at: https://stefvanbuuren.name/fimd/ (Accessed: 04/10/2025).\n"
      ],
      "metadata": {
        "id": "P3Sxs8A_3-ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.1.1**"
      ],
      "metadata": {
        "id": "oRj2lOE-raGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to count rows that are none or null\n",
        "def count_empty_rows(df):\n",
        "  # Filter rows where 'text' column is null or an empty string\n",
        "  null_or_empty_text_count = df.filter(\n",
        "    col(\"text\").isNull() | (trim(col(\"text\")) == \"\")\n",
        "  ).count()\n",
        "  return null_or_empty_text_count\n",
        "#show count of empty rows in text field before filling in with no review\n",
        "print(f\"The number of empty rows in the text column before replacing with 'no review' is: {count_empty_rows(df_review)}\")"
      ],
      "metadata": {
        "id": "4nA-TOEw_6yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace null values with \"No review\"\n",
        "df_review = df_review.fillna({'text': 'no review'})\n",
        "\n",
        "# Replace empty strings (after trimming whitespace) with \"No review\"\n",
        "df_review = df_review.withColumn(\"text\",\n",
        "    when(trim(col(\"text\")) == \"\", lit(\"no review\")).otherwise(col(\"text\"))\n",
        ")\n",
        "\n",
        "# Show the count of text column which is empty or null to verify the changes\n",
        "print(f\"The number of empty rows in the text column after replacing with no review is: {count_empty_rows(df_review)}\")\n",
        "\n",
        "# Count rows where the 'text' column is 'no review'\n",
        "no_review_count = df_review.filter(col(\"text\") == \"no review\").count()\n",
        "\n",
        "print(f\"The number of reviews with 'no review' in the text column is: {no_review_count}\")\n"
      ],
      "metadata": {
        "id": "NE7YPNsBmNDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.1.2**"
      ],
      "metadata": {
        "id": "PGG6r7-otTdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the legacy time parser policy to corrected to handle potential parsing issues\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
        "\n",
        "# Convert 'time' from epoch milliseconds to a timestamp, then to a date string\n",
        "df_review = df_review.withColumn(\n",
        "    \"newtime\",\n",
        "    to_date(from_unixtime(col(\"time\") / 1000).cast(\"timestamp\"), \"yyyy-MM-dd\")\n",
        ")\n",
        "\n",
        "# Display the first few rows with the new column\n",
        "df_review.select(\"user_id\",\"name\",\"rating\",\"time\", \"newtime\",\"text\",\"gmap_id\").show(5)"
      ],
      "metadata": {
        "id": "e702ILlLH2oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2**"
      ],
      "metadata": {
        "id": "s9SOA54GxrVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2.1**"
      ],
      "metadata": {
        "id": "73LlzorL555T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of reviews per unique gmap_id\n",
        "reviews_per_gmap = df_review.groupBy(\"gmap_id\").agg(count(\"*\").alias(\"review_count\"))\n",
        "\n",
        "# Cast the review_count to float type\n",
        "reviews_per_gmap = reviews_per_gmap.withColumn(\"review_count\", col(\"review_count\").cast(DoubleType()))\n",
        "\n",
        "# Show the top 5 results\n",
        "print(\"Number of reviews per unique gmap_id (Top 5):\")\n",
        "reviews_per_gmap.orderBy(col(\"review_count\").desc()).show(5)"
      ],
      "metadata": {
        "id": "aPzG0khxewGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2.2**"
      ],
      "metadata": {
        "id": "smGy0nJf6Akh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert PySpark DataFrame to pandas DataFrame\n",
        "df = df_review.toPandas()\n",
        "\n",
        "# Create 'review_time' column at the hour level\n",
        "df['review_time'] = pd.to_datetime(df['time'], unit='ms').dt.hour\n",
        "\n",
        "# Display the first 5 rows of the pandas DataFrame\n",
        "print(\"Pandas DataFrame with 'review_time' column (Top 5):\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "GKKiQDDB4JRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2.3**"
      ],
      "metadata": {
        "id": "br0XEAsR6DPr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2082982c"
      },
      "source": [
        "# Analyze the distribution of reviews by hour\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='review_time', palette='viridis')\n",
        "plt.title('Distribution of Review Times by Hour')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "\n",
        "# Analyze the number of businesses reviewed at different times\n",
        "unique_gmap_ids_per_hour = df.groupby('review_time')['gmap_id'].nunique().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=unique_gmap_ids_per_hour, x='review_time', y='gmap_id', palette='viridis')\n",
        "plt.title('Number of Unique Businesses Reviewed per Hour')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Number of Unique Businesses')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe1c49ca"
      },
      "source": [
        "### Insights from Visualizations\n",
        "\n",
        "The visualizations above provide insights into the review patterns based on the time of day:\n",
        "\n",
        "**Distribution of Review Times by Hour:**\n",
        "- The first plot shows the total number of reviews submitted at each hour of the day.\n",
        "- We can observe that there is a peak in reviews during certain hours, indicating when users are most active in leaving feedback. This could be related to business operating hours, user routines, or other factors.\n",
        "\n",
        "**Number of Unique Businesses Reviewed per Hour:**\n",
        "- The second plot shows the number of distinct businesses (identified by `gmap_id`) that received at least one review during each hour.\n",
        "- This helps understand if reviews are spread across many businesses throughout the day or concentrated on a few during specific times.\n",
        "\n",
        "Further analysis could involve looking at the average rating per hour, the types of businesses reviewed at different times, or correlating review times with other factors in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3**"
      ],
      "metadata": {
        "id": "QdkOlMSFENva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3.1**"
      ],
      "metadata": {
        "id": "UtIiytTrERdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rename the name column in the meta_data to avoid duplicate name error with the reviews data set\n",
        "df_meta = df_meta.withColumnRenamed(\"name\", \"business_name\")\n",
        "\n",
        "# Join the two DataFrames on the 'gmap_id' column\n",
        "joined_df = df_review.join(df_meta, on=\"gmap_id\", how=\"inner\")\n",
        "\n",
        "# Show the schema and some rows of the joined DataFrame\n",
        "print(\"Joined DataFrame Schema:\")\n",
        "joined_df.printSchema()\n",
        "\n",
        "print(\"Joined DataFrame (Top 5):\")\n",
        "joined_df.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "480yoGtO52Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the day of the week from the 'newtime' column\n",
        "joined_df_with_day = joined_df.withColumn(\"day_of_week\", date_format(col(\"newtime\"), \"E\"))\n",
        "\n",
        "# Group by day of the week and count the reviews\n",
        "reviews_by_day = joined_df_with_day.groupBy(\"day_of_week\").count()\n",
        "\n",
        "# Define the order of the days of the week for plotting\n",
        "day_order = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
        "\n",
        "# Convert to pandas DataFrame for plotting\n",
        "reviews_by_day_pd = reviews_by_day.toPandas()\n",
        "\n",
        "# Sort the pandas DataFrame by the defined day order\n",
        "reviews_by_day_pd['day_of_week'] = pd.Categorical(reviews_by_day_pd['day_of_week'], categories=day_order, ordered=True)\n",
        "reviews_by_day_pd = reviews_by_day_pd.sort_values('day_of_week')\n",
        "\n",
        "\n",
        "# Plot the results as a line chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=reviews_by_day_pd, x='day_of_week', y='count', marker='o')\n",
        "plt.title('Number of Reviews per Day of the Week (Joined Data)')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w54ao2_eCUEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Observations**\n",
        "\n",
        "The day of the week that has the highest reviews is Sunday, Friday has the lowest reviews."
      ],
      "metadata": {
        "id": "Gre5uj3qKl6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3.2**"
      ],
      "metadata": {
        "id": "9Ei1XwjWKUo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the day with the maximum number of reviews\n",
        "peak_day = reviews_by_day_pd.loc[reviews_by_day_pd['count'].idxmax()]['day_of_week']\n",
        "print(f\"The workday with the most reviews is: {peak_day}\")\n",
        "\n",
        "# Filter the joined DataFrame for the peak day\n",
        "peak_day_df = joined_df_with_day.filter(col(\"day_of_week\") == peak_day)\n",
        "\n",
        "# Group by business name and category and calculate the average rating\n",
        "avg_rating_by_business = peak_day_df.groupBy(\"business_name\", \"category\").agg(avg(\"rating\").alias(\"average_rating\"))\n",
        "\n",
        "# Find the business(es) with the highest average rating on the peak day\n",
        "highest_rated_businesses = avg_rating_by_business.orderBy(col(\"average_rating\").desc()).limit(5)\n",
        "\n",
        "print(f\"\\nBusinesses with the highest average rating on {peak_day} (Top 5):\")\n",
        "highest_rated_businesses.show(truncate=False)"
      ],
      "metadata": {
        "id": "Oliw0wxTDMIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3.3**"
      ],
      "metadata": {
        "id": "lyKfEILCEeiL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0549d5b"
      },
      "source": [
        "# Group by category and count the number of businesses\n",
        "category_counts = joined_df.groupBy(\"category\").count()\n",
        "\n",
        "# Order by count in descending order\n",
        "category_counts = category_counts.orderBy(col(\"count\").desc())\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "category_counts_pd = category_counts.toPandas()\n",
        "\n",
        "# Select the top 10 categories\n",
        "top_10_categories = category_counts_pd.head(10)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(data=top_10_categories, x='category', y='count', palette='viridis')\n",
        "plt.title('Top 10 Business Categories by Count')\n",
        "plt.xlabel('Business Category')\n",
        "plt.ylabel('Number of Businesses')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e45abe92"
      },
      "source": [
        "# Get the list of top 10 categories\n",
        "top_categories_list = top_10_categories['category'].tolist()\n",
        "\n",
        "# Filter the joined DataFrame to include only the top 10 categories\n",
        "filtered_df = joined_df_with_day.filter(col(\"category\").isin(top_categories_list))\n",
        "\n",
        "# Extract the hour of the day from the 'newtime' column\n",
        "filtered_df = filtered_df.withColumn(\"review_hour\", date_format(col(\"newtime\"), \"H\"))\n",
        "\n",
        "# Show the schema and some rows of the filtered DataFrame with the new column\n",
        "print(\"Filtered DataFrame Schema:\")\n",
        "filtered_df.printSchema()\n",
        "\n",
        "print(\"Filtered DataFrame (Top 5):\")\n",
        "filtered_df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ece5d5c"
      },
      "source": [
        "# Group by category and review_hour and count the number of reviews\n",
        "reviews_by_category_hour = filtered_df.groupBy(\"category\", \"review_hour\").count()\n",
        "\n",
        "# Find the peak review hour for each category\n",
        "\n",
        "window_spec = Window.partitionBy(\"category\").orderBy(col(\"count\").desc())\n",
        "\n",
        "ranked_reviews = reviews_by_category_hour.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "peak_hours_per_category = ranked_reviews.filter(col(\"rank\") == 1)\n",
        "\n",
        "# Display the peak review hour for each of the top 10 categories\n",
        "print(\"Peak review hour for each of the top 10 categories:\")\n",
        "peak_hours_per_category.orderBy(col(\"count\").desc()).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56150fc3"
      },
      "source": [
        "## Analyze average ratings by category\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb3b8c25"
      },
      "source": [
        "# Group by category and calculate the average rating\n",
        "avg_rating_by_category = joined_df.groupBy(\"category\").agg(avg(\"rating\").alias(\"average_rating\"))\n",
        "\n",
        "# Order by average_rating in descending order\n",
        "avg_rating_by_category = avg_rating_by_category.orderBy(col(\"average_rating\").desc())\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "avg_rating_by_category_pd = avg_rating_by_category.toPandas()\n",
        "\n",
        "# Select the top 10 categories by average rating\n",
        "top_10_avg_rating_categories = avg_rating_by_category_pd.head(10)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.barplot(data=top_10_avg_rating_categories, x='category', y='average_rating', palette='viridis')\n",
        "plt.title('Top 10 Business Categories by Average Rating')\n",
        "plt.xlabel('Business Category')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ac079f"
      },
      "source": [
        "**Analysis Summary:**\n",
        "\n",
        "Business Categories, Peak Hours, and Average Ratings\n",
        "This analysis explored review patterns based on business categories, focusing on the distribution of businesses, peak review hours for top categories, and average ratings across categories.\n",
        "\n",
        "**Distribution of Businesses Across Categories:**\n",
        "\n",
        "Based on the analysis of business categories, the most frequent categories in the dataset are:\n",
        "\n",
        "['Shopping mall']\n",
        "['Fast food restaurant', 'Breakfast restaurant', 'Hamburger restaurant', 'Restaurant', 'Sandwich shop']\n",
        "['Department store', 'Clothing store', 'Craft store', 'Shoe store', 'Sporting goods store']\n",
        "['Grocery store', 'Grocery delivery service']\n",
        "['Mexican restaurant']\n",
        "['Fast food restaurant', 'Breakfast restaurant', 'Burger joint', 'Restaurant', 'Sandwich shop']\n",
        "['Restaurant']\n",
        "['Warehouse store', 'Department store']\n",
        "['American restaurant']\n",
        "['Gas station']\n",
        "The bar plot titled \"Top 10 Business Categories by Count\" visually represents this distribution, clearly showing the categories with the highest number of associated businesses.\n",
        "\n",
        "**Peak Review Hours for Top Categories:**\n",
        "\n",
        "The analysis of peak review hours for the top 10 business categories revealed that for all of these categories, the hour with the highest number of reviews is '0' (which likely corresponds to midnight or the beginning of the day, depending on the exact time data representation).\n",
        "\n",
        "**Average Ratings by Category:**\n",
        "\n",
        "The calculation of average ratings by category identified the categories with the highest average ratings. The top 10 categories by average rating are:\n",
        "\n",
        "['Beauty salon', 'Hair care', 'Hair salon', 'Nail salon']\n",
        "['Snowboard shop', 'Clothing store', 'Shoe store', 'Ski shop', 'Sporting goods store', 'Store']\n",
        "['Horseback riding service', 'Bed & breakfast', 'Cabin rental agency', 'Cottage', 'Guest house']\n",
        "['Pediatric dentist', 'Dentist']\n",
        "['Pediatrician']\n",
        "['Surgeon', 'Doctor', 'Medical spa']\n",
        "['Waterfall', 'Tourist attraction']\n",
        "['Rafting', 'Raft trip outfitter', 'Scenic spot']\n",
        "['Dogsled ride service', 'Helicopter tour agency', 'Tour operator']\n",
        "['Leather goods store', 'Store']\n",
        "The bar plot titled \"Top 10 Business Categories by Average Rating\" illustrates these top-rated categories and their corresponding average ratings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all review text into a single string in PySpark\n",
        "all_reviews_text_df = joined_df.select(concat_ws(\" \", col(\"text\"))).collect()\n",
        "all_reviews_text = all_reviews_text_df[0][0] if all_reviews_text_df else \"\"\n",
        "\n",
        "\n",
        "# Remove punctuation and convert to lowercase using PySpark functions\n",
        "words_df = joined_df.select(lower(regexp_replace(col(\"text\"), r'[^\\w\\s]', '')).alias(\"text\"))\n",
        "words_df = words_df.select(explode(split(col(\"text\"), \"\\s+\")).alias(\"word\"))\n",
        "\n",
        "# Remove common English stop words using PySpark\n",
        "stop_words = set([\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
        "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"herself\",\n",
        "    \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\",\n",
        "    \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\",\n",
        "    \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\",\n",
        "    \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
        "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
        "    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
        "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
        "    \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
        "    \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
        "    \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"no\", \"review\"\n",
        "])\n",
        "\n",
        "words_df = words_df.filter(~col(\"word\").isin(stop_words)).filter(length(col(\"word\")) > 1)\n",
        "\n",
        "\n",
        "# Count the frequency of each word using PySpark\n",
        "word_counts_df = words_df.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
        "\n",
        "# Get the top 30 most common words\n",
        "top_30_words = word_counts_df.limit(30).collect()\n",
        "\n",
        "print(\"Top 30 most common words:\")\n",
        "for row in top_30_words:\n",
        "    print(f\"{row['word']}: {row['count']}\")\n",
        "\n",
        "# Generate word clouds for each year\n",
        "# Extract the year from the 'newtime' column\n",
        "reviews_with_year = joined_df.withColumn(\"review_year\", year(col(\"newtime\")))\n",
        "\n",
        "# Group by year and concatenate text for each year\n",
        "reviews_by_year_df = reviews_with_year.groupBy(\"review_year\").agg(concat_ws(\" \", collect_list(col(\"text\"))).alias(\"all_text\"))\n",
        "\n",
        "# Collect the data for word cloud generation\n",
        "reviews_by_year_pd = reviews_by_year_df.toPandas()"
      ],
      "metadata": {
        "id": "wUgBkQKWuWMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort reviews by year before plotting the word clouds\n",
        "reviews_by_year_pd.sort_values(by='review_year', inplace=True)\n",
        "reviews_by_year_pd.head()"
      ],
      "metadata": {
        "id": "3hMxZDzmoq8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate word cloud for each year\n",
        "num_years = len(reviews_by_year_pd)\n",
        "num_cols = 2\n",
        "num_rows = (num_years + num_cols - 1) // num_cols # Calculate number of rows needed\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10 * num_cols, 5 * num_rows))\n",
        "axes = axes.flatten() # Flatten the array of axes for easy iteration\n",
        "\n",
        "for i, (index, row) in enumerate(reviews_by_year_pd.iterrows()):\n",
        "    review_year_val = row['review_year']\n",
        "    text = row['all_text']\n",
        "\n",
        "    if text: # Check if there is any text for the year\n",
        "        # Remove punctuation and convert to lowercase\n",
        "        text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
        "        words = text.split()\n",
        "        words = [word for word in words if word not in stop_words and len(word) > 1]\n",
        "        text = \" \".join(words)\n",
        "\n",
        "        if text: # Check if there are any words left after removing stop words\n",
        "            wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "            axes[i].imshow(wordcloud, interpolation='bilinear')\n",
        "            axes[i].axis('off')\n",
        "            axes[i].set_title(f'Word Cloud for {review_year_val}')\n",
        "        else:\n",
        "            axes[i].axis('off') # Hide axis if no words to display\n",
        "            axes[i].set_title(f'No significant words for {review_year_val}')\n",
        "    else:\n",
        "        axes[i].axis('off') # Hide axis if no text for the year\n",
        "        axes[i].set_title(f'No reviews for {review_year_val}')\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uqufND1Fx74k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33b12f9d"
      },
      "source": [
        "### Insights from Top Words by Year\n",
        "\n",
        "Analyzing the top words for each year provides a clearer view of the review content and highlights shifts in focus over time:\n",
        "\n",
        "*   **Consistency of Core Themes:** Words like \"great,\" \"good,\" \"food,\" \"place,\" and \"service\" consistently appear in the top words across many years, indicating that these are fundamental aspects of the reviewed businesses that users frequently comment on.\n",
        "*   **Growth in Review Volume:** The increasing counts for the top words over the years (especially noticeable from 2016 onwards) directly reflect the significant growth in the number of reviews in the dataset during this period.\n",
        "*   **Dominance of Positive Language:** The prevalence of positive words like \"great,\" \"good,\" and \"amazing\" in the top lists suggests a generally positive sentiment within the reviews, based on the most frequent terms.\n",
        "*   **Specific Mentions:** While the very top words are quite general, looking beyond the top 5 or 10 for each year might reveal more specific keywords related to particular types of businesses or events that were prominent in those years.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.5**"
      ],
      "metadata": {
        "id": "Ah_-9i6IDq67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of unique reviewers per business\n",
        "unique_reviewers_per_business = joined_df.groupBy(\"business_name\", \"gmap_id\").agg(countDistinct(\"user_id\").alias(\"unique_reviewer_count\"))\n",
        "\n",
        "print(\"Number of unique reviewers per business (Top 10):\")\n",
        "unique_reviewers_per_business.orderBy(col(\"unique_reviewer_count\").desc()).show(10, truncate=False)\n",
        "\n",
        "# Determine the number of unique reviewers per category\n",
        "unique_reviewers_per_category = joined_df.groupBy(\"category\").agg(countDistinct(\"user_id\").alias(\"unique_reviewer_count\"))\n",
        "\n",
        "print(\"Number of unique reviewers per category (Top 10):\")\n",
        "unique_reviewers_per_category.orderBy(col(\"unique_reviewer_count\").desc()).show(10, truncate=False)\n",
        "\n",
        "# Analyze temporal patterns of reviews by year\n",
        "reviews_by_year = joined_df.withColumn(\"review_year\", year(col(\"newtime\"))).groupBy(\"review_year\").count().orderBy(\"review_year\")\n",
        "\n",
        "print(\"Number of reviews per year:\")\n",
        "reviews_by_year.show(truncate=False)\n",
        "\n",
        "# Analyze temporal patterns of reviews by month\n",
        "reviews_by_month = joined_df.withColumn(\"review_month\", month(col(\"newtime\"))).groupBy(\"review_month\").count().orderBy(\"review_month\")\n",
        "\n",
        "print(\"Number of reviews per month:\")\n",
        "reviews_by_month.show(truncate=False)\n",
        "\n",
        "# Analyze temporal patterns of reviews by day of the week\n",
        "reviews_by_dayofweek = joined_df.withColumn(\"review_dayofweek\", dayofweek(col(\"newtime\"))).groupBy(\"review_dayofweek\").count().orderBy(\"review_dayofweek\")\n",
        "\n",
        "print(\"Number of reviews per day of the week (1 = Sunday, 7 = Saturday):\")\n",
        "reviews_by_dayofweek.show(truncate=False)"
      ],
      "metadata": {
        "id": "X9W8FCkpyf-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights from Unique Reviewers and Temporal Patterns\n",
        "\n",
        "**Unique Reviewers per Business and Category:**\n",
        "\n",
        "*   The analysis of unique reviewers per business reveals the businesses that have attracted the largest number of distinct individuals to leave reviews. The top businesses with the highest unique reviewer counts are:\n",
        "    *   Moose's Tooth Pub & Pizzeria\n",
        "    *   Dimond Center\n",
        "    *   Walmart Supercenter (multiple locations appear in the top 10)\n",
        "    *   Costco Wholesale (multiple locations appear in the top 10)\n",
        "    *   Anchorage 5th Avenue Mall\n",
        "    *   49th State Brewing - Anchorage\n",
        "    *   Tikahtnu Commons\n",
        "\n",
        "*   Similarly, examining unique reviewers per category highlights the business categories that receive reviews from the broadest range of users. The top categories by unique reviewer count are:\n",
        "    *   ['Shopping mall']\n",
        "    *   ['Department store', 'Clothing store', 'Craft store', 'Discount store', 'Electronics store', 'Grocery store', 'Home goods store', 'Sporting goods store', 'Supermarket', 'Toy store']\n",
        "    *   ['Fast food restaurant', 'Breakfast restaurant', 'Coffee shop', 'Hamburger restaurant', 'Restaurant', 'Sandwich shop']\n",
        "    *   ['Grocery store', 'Grocery delivery service']\n",
        "    *   ['Mexican restaurant']\n",
        "    *   ['Warehouse store', 'Department store']\n",
        "    *   ['Fast food restaurant', 'Breakfast restaurant', 'Burrito restaurant', 'Lunch restaurant', 'Takeout Restaurant', 'Mexican restaurant', 'Restaurant', 'Taco restaurant', 'Tex-Mex restaurant', 'Vegetarian restaurant']\n",
        "    *   ['Restaurant']\n",
        "    *   ['American restaurant']\n",
        "    *   ['Grocery store', 'Propane supplier']\n",
        "\n",
        "These findings indicate that large retail centers, popular restaurants (especially fast food and casual dining), and general merchandise stores tend to attract reviews from a wider customer base.\n",
        "\n",
        "**Seasonal Patterns of Reviews:**\n",
        "\n",
        "*   **Reviews by Year:** The number of reviews has significantly increased over the years, with a substantial jump starting around 2016 and peaking in 2019 and 2020. This suggests a growing trend of users leaving reviews or an expansion of the dataset's coverage in recent years.\n",
        "\n",
        "* **Reviews by Month:** The months June to September have the highest review counts > 45k. This shows that most customers tend to spend more time during summer months shopping at large retail centers, restuarants and general merchandise shops. It might also be reflective customer behavioral patterns during different times of the year.\n",
        "\n",
        "* **Reviews by day of week** The days (saturday and sunday) have more review counts > 75k, compared to the rest of the days an indicator that customers tend to spend more times shopping during the weekend that over the rest of the days.\n"
      ],
      "metadata": {
        "id": "uMvJ65wBDlE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.6**"
      ],
      "metadata": {
        "id": "Jyv13UA09Paa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.6.1**"
      ],
      "metadata": {
        "id": "U5SdPuMk-x0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common approach for building a recommendation system for businesses based on customer reviews is using Collaborative Filtering. This method relies on the idea that users who liked similar items in the past will like similar items in the future. The following steps were used.\n",
        "\n",
        "a. Create User-Item Matrix:\n",
        "\n",
        "o Construct a matrix where rows represent users, columns represent businesses, and the values are the ratings given by the user to the business.This matrix will be sparse (most users haven't reviewed most businesses).\n",
        "\n",
        "b. Choose a Similarity Metric:\n",
        "\n",
        "o Select a metric to measure the similarity between users or between businesses. Common metrics include:\n",
        "\n",
        "Cosine Similarity: Measures the cosine of the angle between two vectors. Useful for sparse data.\n",
        "\n",
        "Pearson Correlation: Measures the linear correlation between two sets of data. Less sensitive to differences in rating scales.\n",
        "\n",
        "c. Apply KNN Algorithm: o User-Based Collaborative Filtering: To recommend businesses to a target user, find the 'K' most similar users based on their past ratings. Then, recommend businesses that these similar users liked but the target user hasn't reviewed yet.\n",
        "\n",
        "o Item-Based Collaborative Filtering: To recommend businesses similar to one the target user liked, find the 'K' most similar businesses based on the ratings they received from all users. Recommend these similar businesses.\n",
        "\n",
        "d. Generate Recommendations:\n",
        "\n",
        "o Based on the chosen collaborative filtering approach (user-based or item-based) and the identified nearest neighbours, predict the potential rating a user would give to unreviewed businesses.\n",
        "\n",
        "o Recommend the businesses with the highest predicted ratings.\n",
        "\n",
        "e. Evaluation:\n",
        "\n",
        "o Split the data into training and testing sets.\n",
        "\n",
        "o Use evaluation metrics like RMSE (Root Mean Squared Error) for rating prediction or Precision/Recall for recommendation accuracy to assess the performance of the model."
      ],
      "metadata": {
        "id": "Tii81soO-0sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.6.2**"
      ],
      "metadata": {
        "id": "0NtvmG0C-uKx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af38ad06"
      },
      "source": [
        "# Create a user-item matrix using the relevant columns from the joined DataFrame\n",
        "# Select and cast the necessary columns\n",
        "ratings_df = joined_df.select(\n",
        "    col(\"user_id\").cast(IntegerType()).alias(\"userId\"),\n",
        "    col(\"gmap_id\").alias(\"itemId\"),\n",
        "    col(\"rating\").cast(DoubleType()).alias(\"rating\")\n",
        ")\n",
        "\n",
        "# Drop rows with null values in the selected columns\n",
        "ratings_df = ratings_df.dropna(subset=[\"userId\", \"itemId\", \"rating\"])\n",
        "\n",
        "# Convert item IDs to numerical IDs for ALS\n",
        "# Hash the string IDs\n",
        "ratings_df = ratings_df.withColumn(\"itemIdNumeric\", hash(col(\"itemId\")))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "(training, test) = ratings_df.randomSplit([0.8, 0.2], seed=123)\n",
        "\n",
        "# Build the recommendation model using ALS (Alternating Least Squares)\n",
        "# ALS is a common collaborative filtering algorithm suitable for sparse data\n",
        "als = ALS(maxIter=5, regParam=0.09, rank=20, userCol=\"userId\", itemCol=\"itemIdNumeric\", ratingCol=\"rating\",\n",
        "          coldStartStrategy=\"drop\")\n",
        "model = als.fit(training)\n",
        "\n",
        "# Evaluate the model by computing the RMSE on the test data\n",
        "predictions = model.transform(test)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "                                predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root-mean-square error = {rmse}\")\n",
        "\n",
        "# Generate top 10 business recommendations for each user\n",
        "userRecs = model.recommendForAllUsers(10)\n",
        "\n",
        "# Show the recommendations for a few users\n",
        "print(\"Top 10 recommendations for users:\")\n",
        "userRecs.show(5, truncate=False)\n",
        "\n",
        "# Generate top 10 user recommendations for each business\n",
        "itemRecs = model.recommendForAllItems(10)\n",
        "\n",
        "# Show the recommendations for a few businesses\n",
        "print(\"Top 10 user recommendations for businesses:\")\n",
        "itemRecs.show(5, truncate=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.7**"
      ],
      "metadata": {
        "id": "kSfKc7-f__gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.7.1**"
      ],
      "metadata": {
        "id": "vGWISAV8AGHL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44fae181"
      },
      "source": [
        "# Group by category and calculate the average rating\n",
        "avg_rating_by_category = joined_df.groupBy(\"category\").agg(avg(\"rating\").alias(\"average_rating\"))\n",
        "\n",
        "# Order by average_rating in descending order\n",
        "avg_rating_by_category = avg_rating_by_category.orderBy(col(\"average_rating\").desc())\n",
        "\n",
        "# Convert to pandas DataFrame for plotting\n",
        "avg_rating_by_category_pd = avg_rating_by_category.toPandas()\n",
        "\n",
        "# Select the top 20 categories by average rating for better visualization\n",
        "top_20_avg_rating_categories = avg_rating_by_category_pd.head(20)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.barplot(data=top_20_avg_rating_categories, x='category', y='average_rating', palette='viridis')\n",
        "plt.title('Top 20 Business Categories by Average Rating')\n",
        "plt.xlabel('Business Category')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the categories with the lowest average ratings\n",
        "bottom_20_avg_rating_categories = avg_rating_by_category_pd.tail(20)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.barplot(data=bottom_20_avg_rating_categories, x='category', y='average_rating', palette='viridis')\n",
        "plt.title('Bottom 20 Business Categories by Average Rating')\n",
        "plt.xlabel('Business Category')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z2CfekSh_tIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7afc34b1"
      },
      "source": [
        "### Insights from Relationship between Rating and Business Categories\n",
        "\n",
        "The visualizations above illustrate the relationship between business categories and their average ratings.\n",
        "\n",
        "**Top 20 Business Categories by Average Rating:**\n",
        "\n",
        "The first bar plot displays the top 20 business categories with the highest average ratings. Observing this plot, we can identify categories that consistently receive positive reviews from users. These might include niche services, specialized shops, or attractions that cater to specific interests and potentially exceed customer expectations within those areas. It's worth noting that some categories might have a smaller number of reviews contributing to their high average, which could make the average less representative.\n",
        "\n",
        "**Bottom 20 Business Categories by Average Rating:**\n",
        "\n",
        "The second bar plot shows the bottom 20 business categories with the lowest average ratings. This plot helps pinpoint categories that tend to receive lower ratings. These could be types of businesses that are more prone to customer complaints, have inherent challenges in meeting expectations, or face strong competition leading to critical reviews. Understanding these categories can be valuable for businesses within them to identify areas for improvement.\n",
        "\n",
        "**General Observations:**\n",
        "\n",
        "*   The plots reveal a variation in average ratings across different business categories.\n",
        "*   Some categories seem to consistently perform better in terms of customer satisfaction (based on average rating) than others.\n",
        "*   To gain deeper insights, it would be beneficial to also consider the number of reviews for each category when interpreting the average ratings, as categories with very few reviews might have skewed averages.\n",
        "*   Further analysis could involve examining the distribution of ratings within specific categories or exploring the common themes in the reviews for high- and low-rated categories."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.7.2**"
      ],
      "metadata": {
        "id": "e4y9yON4EDDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a threshold for low ratings (e.g., ratings less than or equal to 2)\n",
        "low_rating_threshold = 2\n",
        "\n",
        "# Filter the joined DataFrame to get reviews with low ratings\n",
        "low_rated_reviews = joined_df.filter(col(\"rating\") <= low_rating_threshold)\n",
        "\n",
        "# Count the number of low-rated reviews\n",
        "num_low_rated_reviews = low_rated_reviews.count()\n",
        "\n",
        "print(f\"Number of reviews with a rating of {low_rating_threshold} or less: {num_low_rated_reviews}\")\n",
        "\n",
        "# Extract the text from the low-rated reviews and combine it into a single string\n",
        "low_rated_text_df = low_rated_reviews.select(concat_ws(\" \", col(\"text\"))).collect()\n",
        "low_rated_text = low_rated_text_df[0][0] if low_rated_text_df else \"\"\n",
        "\n",
        "# Process the text: remove punctuation, convert to lowercase, and split into words\n",
        "if low_rated_text:\n",
        "    low_rated_words = low_rated_reviews.select(lower(regexp_replace(col(\"text\"), r'[^\\w\\s]', '')).alias(\"text\"))\n",
        "    low_rated_words = low_rated_words.select(explode(split(col(\"text\"), \"\\s+\")).alias(\"word\"))\n",
        "\n",
        "    # Remove common English stop words and words with length less than or equal to 1\n",
        "    stop_words_extended = stop_words.union({\"no\", \"review\"}) # Add \"no\" and \"review\" to stop words\n",
        "    low_rated_words = low_rated_words.filter(~col(\"word\").isin(stop_words_extended)).filter(length(col(\"word\")) > 1)\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    low_rated_word_counts_df = low_rated_words.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
        "\n",
        "    # Get the top 30 most common words in low-rated reviews\n",
        "    top_30_low_rated_words = low_rated_word_counts_df.limit(30).collect()\n",
        "\n",
        "    print(\"\\nTop 30 most common words in low-rated reviews:\")\n",
        "    for row in top_30_low_rated_words:\n",
        "        print(f\"{row['word']}: {row['count']}\")\n",
        "else:\n",
        "    print(\"\\nNo low-rated reviews found to analyze text.\")"
      ],
      "metadata": {
        "id": "kHfAxoveGKvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f4f3815"
      },
      "source": [
        "### **Analysis of Low-Rated Reviews**\n",
        "\n",
        "Based on the analysis of reviews with a rating of 2 or less:\n",
        "\n",
        "- There are **36884** reviews with a low rating.\n",
        "- The most common words appearing in these low-rated reviews include:\n",
        "\n",
        "\n",
        "**food**: This suggests a product indicating poor food quality or service\n",
        "\n",
        "**service**: This may relate to the quality of service, mostly poor services offered.\n",
        "\n",
        "**get**: This could relate to difficulty in receiving service or products\n",
        "\n",
        "**time**: This suggests that issues related to waiting times or the duration of service might be a common complaint.\n",
        "\n",
        "**like**: Can be used in various contexts, but in negative reviews, it might express disappointment or unfavorable comparisons.\n",
        "\n",
        "**bad**: A direct expression of dissatisfaction.\n",
        "\n",
        "**never**: Used to emphasize a consistently negative experience.\n",
        "\n",
        "**store**: Many low ratings are for retail establishments.\n",
        "\n",
        "**went**: Describes negative experiences encountered during a visit.\n",
        "\n",
        "**dont**: Indicates negative experiences or lack of something expected.\n",
        "\n",
        "These words collectively suggest that common reasons for low ratings include issues with service speed and efficiency, poor customer service and communication, problems with products or work quality, and negative experiences related to specific types of businesses like restaurants and retail stores."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.8**"
      ],
      "metadata": {
        "id": "a0DnpY96QgaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.8.1**"
      ],
      "metadata": {
        "id": "u_QVVynzUcVk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74e87f3e"
      },
      "source": [
        "# Sort reviews for each business by newtime\n",
        "window_spec_business = Window.partitionBy(\"gmap_id\").orderBy(asc(\"newtime\"))\n",
        "sorted_reviews_by_business = joined_df.withColumn(\"row_number\", rank().over(window_spec_business)).drop(\"row_number\")\n",
        "\n",
        "# Group by user_id and collect the list of business names\n",
        "user_business_list_df = sorted_reviews_by_business.groupBy(\"user_id\").agg(collect_list(\"business_name\").alias(\"business_list\"))\n",
        "\n",
        "# Convert the result to a list of (user_id, business_list) tuples (optional, for display)\n",
        "user_business_list = user_business_list_df.collect()\n",
        "\n",
        "# Display the result for a few users\n",
        "print(\"Business list for each user (first 20 users):\")\n",
        "for row in user_business_list[:20]:\n",
        "    print(f\"User ID: {row['user_id']}, Businesses: {row['business_list']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.8.2**"
      ],
      "metadata": {
        "id": "w35DT-eqWISM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check for repeated business names in a list\n",
        "def has_repeated_businesses(business_list):\n",
        "    if not business_list:\n",
        "        return False\n",
        "    # Convert the list to a set to find unique businesses\n",
        "    unique_businesses = set(business_list)\n",
        "    # If the number of unique businesses is less than the total number of reviews,\n",
        "    # it means there are repeated business names\n",
        "    return len(unique_businesses) < len(business_list)\n",
        "\n",
        "# Count the number of users with repeated business names in their list\n",
        "users_with_repeated_businesses_count = sum(\n",
        "    1 for user_id, business_list in user_business_list if has_repeated_businesses(business_list)\n",
        ")\n",
        "\n",
        "print(f\"Number of users with repeated business names in their review history: {users_with_repeated_businesses_count}\")"
      ],
      "metadata": {
        "id": "gKYBp2ciQi7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify and print duplicated business names and their counts for each reviewer\n",
        "print(\"\\nDuplicated business names and their counts for each user:\")\n",
        "count = 0\n",
        "for user_id, business_list in user_business_list:\n",
        "    if has_repeated_businesses(business_list):\n",
        "        # Use Counter to find duplicated business names and their counts\n",
        "        business_counts = Counter(business_list)\n",
        "        duplicated_businesses = {business: count for business, count in business_counts.items() if count > 1}\n",
        "        if duplicated_businesses:\n",
        "            print(f\"User ID: {user_id}\")\n",
        "            count += 1\n",
        "            for business, business_count in duplicated_businesses.items():\n",
        "                if count <= 1000:  # Limit to the first 1000 lines\n",
        "                    print(f\"  - {business}: {business_count} times\")\n",
        "                    count += 1\n",
        "                else:\n",
        "                    break # Exit the inner loop if 1000 lines are reached\n",
        "    if count > 1000: # Exit the outer loop if 1000 lines are reached\n",
        "        break"
      ],
      "metadata": {
        "id": "RFm7F0Q8lWRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "329ec989"
      },
      "source": [
        "# Create a new list to store user_id and a list of unique businesses\n",
        "user_business_list_unique = []\n",
        "\n",
        "# Iterate through the original list\n",
        "for user_id, business_list in user_business_list:\n",
        "    # Convert the business list to a set to get unique businesses, then convert back to a list\n",
        "    unique_businesses = list(set(business_list))\n",
        "    # Append the user_id and the list of unique businesses to the new list\n",
        "    user_business_list_unique.append((user_id, unique_businesses))\n",
        "\n",
        "# Display the result for a few users to verify\n",
        "print(\"Business list for each user with duplicates removed (first 20 users):\")\n",
        "for row in user_business_list_unique[:20]:\n",
        "    print(f\"User ID: {row[0]}, Businesses: {row[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.8.3**"
      ],
      "metadata": {
        "id": "bMRLoyi2WNM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf,expr\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Define a UDF to encode business names using hashing\n",
        "@udf(IntegerType())\n",
        "def encode_business_name(business_name):\n",
        "    if business_name is None:\n",
        "        return None\n",
        "    return hash(business_name)"
      ],
      "metadata": {
        "id": "5uR1ONW11ur6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the hash function directly to each element in the business_list column\n",
        "encoded_user_business_list_df = user_business_list_df.withColumn(\n",
        "    \"encoded_business_list\",\n",
        "    # Use transform to apply the hash function to each element in the list\n",
        "    expr(\"transform(business_list, business -> hash(business))\")\n",
        ")\n",
        "\n",
        "# Display the result for a few users to verify\n",
        "print(\"Business list for each user with encoded business names (first 20 users):\")\n",
        "for row in encoded_user_business_list_df.collect()[:20]:\n",
        "    print(f\"User ID: {row['user_id']}, Encoded Businesses: {row['encoded_business_list']}\")"
      ],
      "metadata": {
        "id": "Cfpkobqt10O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_user_business_list_df.columns"
      ],
      "metadata": {
        "id": "ZGHQA3LN17FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "# Group the DataFrame by user_id and collect the unique gmap_id's into a set for fast lookups\n",
        "encoded_user_business_list_series = df.groupby('user_id')['gmap_id'].apply(lambda x: set(x.unique()))\n",
        "\n",
        "# Convert the Series back to a DataFrame with a descriptive column name\n",
        "encoded_user_business_list_df = encoded_user_business_list_series.rename('reviewed_businesses').to_frame()\n",
        "\n",
        "print(\"--- Encoded User-Business List (Sample) ---\")\n",
        "display(encoded_user_business_list_df.head())"
      ],
      "metadata": {
        "id": "LFStD9hz1-qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import display\n",
        "\n",
        "# Create a sample user-item data set for memory efficiency\n",
        "user_sets = encoded_user_business_list_series  # Get user sets\n",
        "\n",
        "sample_size = min(10000, len(user_sets))  # Limit sample size to 10000\n",
        "user_sets_sample = user_sets.sample(n=sample_size, random_state=42)\n",
        "\n",
        "print(f\"Using {sample_size} users for similarity computation (sampled).\")\n",
        "\n",
        "# Create sparse user-item matrix\n",
        "unique_items = sorted({item for items in user_sets_sample for item in items})\n",
        "item_index = {item: i for i, item in enumerate(unique_items)}\n",
        "\n",
        "rows, cols = [], []\n",
        "for row_idx, items in enumerate(user_sets_sample):\n",
        "    cols.extend([item_index[i] for i in items])\n",
        "    rows.extend([row_idx] * len(items))\n",
        "\n",
        "data = np.ones(len(rows), dtype=np.float32)\n",
        "user_item_matrix = csr_matrix(\n",
        "    (data, (rows, cols)), shape=(len(user_sets_sample), len(unique_items))\n",
        ")\n",
        "\n",
        "print(f\"Created sparse matrix: {user_item_matrix.shape[0]} users √ó {user_item_matrix.shape[1]} items\")\n",
        "print(f\"Sparsity: {100 * (1 - user_item_matrix.nnz / (user_item_matrix.shape[0] * user_item_matrix.shape[1])):.2f}%\")\n",
        "\n",
        "# Compute cosine similarity\n",
        "print(\"\\nComputing cosine similarity...\")\n",
        "cosine_sim_matrix = cosine_similarity(user_item_matrix, dense_output=False)\n",
        "\n",
        "# Extract Top-N similar users for each user\n",
        "top_N = 10\n",
        "user_ids = user_sets_sample.index.to_list()\n",
        "similar_users = []\n",
        "\n",
        "print(f\"\\nExtracting top {top_N} similar users for each user...\")\n",
        "\n",
        "# Add progress tracking\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Iterate over each user row in sparse similarity matrix\n",
        "for i in tqdm(range(cosine_sim_matrix.shape[0]), desc=\"Processing users\"):\n",
        "    # Get row as sparse format\n",
        "    row = cosine_sim_matrix.getrow(i)\n",
        "\n",
        "    # Convert sparse row to COO format for efficient access\n",
        "    row_coo = row.tocoo()\n",
        "    sim_scores = list(zip(row_coo.col, row_coo.data))\n",
        "\n",
        "    # Remove self-similarity\n",
        "    sim_scores = [(j, sim) for j, sim in sim_scores if j != i]\n",
        "\n",
        "    # Sort by similarity and select top N\n",
        "    top_neighbors = sorted(sim_scores, key=lambda x: x[1], reverse=True)[:top_N]\n",
        "\n",
        "    # Map back to user IDs\n",
        "    user_i = user_ids[i]\n",
        "    for j, sim in top_neighbors:\n",
        "        similar_users.append({\n",
        "            \"user_a\": user_i,\n",
        "            \"user_b\": user_ids[j],\n",
        "            \"similarity\": float(sim)  # Convert to float for better display\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame\n",
        "topN_df = pd.DataFrame(similar_users)\n",
        "\n",
        "# Display sample results\n",
        "print(f\"\\nTop {top_N} similar users per user (sample):\")\n",
        "display(topN_df.head(20))\n",
        "print(f\"\\nGenerated {len(topN_df):,} user-user similarity pairs across {len(user_ids):,} users.\")\n",
        "\n",
        "# Show summary statistics\n",
        "print(\"\\nSimilarity score statistics:\")\n",
        "print(topN_df['similarity'].describe())"
      ],
      "metadata": {
        "id": "Nd0OJ_lU-ksH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: GPU Jaccard using PyTorch (Works reliably on Colab)\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Install PyTorch if not available\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "except:\n",
        "    !pip install torch\n",
        "\n",
        "def calculate_jaccard_gpu_pytorch(user_business_list_unique, sample_size=10000, batch_size=1000):\n",
        "    \"\"\"\n",
        "    Calculate Jaccard similarity using PyTorch GPU acceleration.\n",
        "    Works reliably on Google Colab with A100.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    user_business_list_unique : list\n",
        "        List of (user_id, business_list) tuples\n",
        "    sample_size : int\n",
        "        Number of users to sample\n",
        "    batch_size : int\n",
        "        Batch size for GPU processing\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame : DataFrame with similarity scores\n",
        "\n",
        "    References:\n",
        "    -----------\n",
        "    Jaccard, P. (1901). \"√âtude comparative de la distribution florale\"\n",
        "    \"\"\"\n",
        "\n",
        "    # Check GPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    if device.type == 'cuda':\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Convert to dictionary format\n",
        "    user_business_dict = {}\n",
        "    for user_id, business_list in user_business_list_unique:\n",
        "        user_business_dict[user_id] = set(business_list)\n",
        "\n",
        "    # Sample users if needed\n",
        "    user_ids = list(user_business_dict.keys())\n",
        "    if len(user_ids) > sample_size:\n",
        "        np.random.seed(42)\n",
        "        user_ids = list(np.random.choice(user_ids, sample_size, replace=False))\n",
        "\n",
        "    print(f\"Processing {len(user_ids)} users...\")\n",
        "\n",
        "    # Create business vocabulary\n",
        "    all_businesses = set()\n",
        "    for uid in user_ids:\n",
        "        all_businesses.update(user_business_dict[uid])\n",
        "\n",
        "    business_to_idx = {b: i for i, b in enumerate(sorted(all_businesses))}\n",
        "    user_to_idx = {u: i for i, u in enumerate(user_ids)}\n",
        "\n",
        "    n_users = len(user_ids)\n",
        "    n_businesses = len(all_businesses)\n",
        "    print(f\"Matrix size: {n_users} users x {n_businesses} businesses\")\n",
        "\n",
        "    # Build sparse representation first (memory efficient)\n",
        "    print(\"Building user-business matrix...\")\n",
        "    rows, cols = [], []\n",
        "    for user_idx, user_id in enumerate(user_ids):\n",
        "        businesses = user_business_dict[user_id]\n",
        "        for business in businesses:\n",
        "            if business in business_to_idx:\n",
        "                rows.append(user_idx)\n",
        "                cols.append(business_to_idx[business])\n",
        "\n",
        "    # Create sparse tensor on GPU\n",
        "    indices = torch.tensor([rows, cols], dtype=torch.long, device=device)\n",
        "    values = torch.ones(len(rows), dtype=torch.float32, device=device)\n",
        "    user_business_matrix = torch.sparse_coo_tensor(\n",
        "        indices, values, (n_users, n_businesses), device=device\n",
        "    ).to_dense()\n",
        "\n",
        "    # Calculate similarities in batches\n",
        "    print(\"Computing Jaccard similarities on GPU...\")\n",
        "    results = []\n",
        "\n",
        "    # Process in batches to manage memory\n",
        "    for start_idx in tqdm(range(0, n_users, batch_size)):\n",
        "        end_idx = min(start_idx + batch_size, n_users)\n",
        "        batch = user_business_matrix[start_idx:end_idx]\n",
        "\n",
        "        # Compute intersection using matrix multiplication\n",
        "        # intersection[i,j] = number of common businesses\n",
        "        intersection = torch.mm(batch, user_business_matrix.t())\n",
        "\n",
        "        # Compute sizes of each set\n",
        "        batch_sizes = batch.sum(dim=1, keepdim=True)\n",
        "        all_sizes = user_business_matrix.sum(dim=1, keepdim=True)\n",
        "\n",
        "        # Compute union = |A| + |B| - |A ‚à© B|\n",
        "        union = batch_sizes + all_sizes.t() - intersection\n",
        "\n",
        "        # Compute Jaccard = |A ‚à© B| / |A ‚à™ B|\n",
        "        # Avoid division by zero\n",
        "        jaccard = torch.where(union > 0,\n",
        "                            intersection / union,\n",
        "                            torch.zeros_like(intersection))\n",
        "\n",
        "        # Extract non-trivial similarities (upper triangle only)\n",
        "        jaccard_np = jaccard.cpu().numpy()\n",
        "\n",
        "        for i in range(jaccard_np.shape[0]):\n",
        "            global_i = start_idx + i\n",
        "            # Only process upper triangle to avoid duplicates\n",
        "            for j in range(global_i + 1, n_users):\n",
        "                sim = jaccard_np[i, j]\n",
        "                if sim > 0:  # Only store non-zero similarities\n",
        "                    results.append({\n",
        "                        'user_a': user_ids[global_i],\n",
        "                        'user_b': user_ids[j],\n",
        "                        'similarity': float(sim)\n",
        "                    })\n",
        "\n",
        "    # Clear GPU memory\n",
        "    del user_business_matrix, intersection, union, jaccard\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run the PyTorch GPU implementation\n",
        "print(\"Starting GPU Jaccard calculation using PyTorch...\")\n",
        "start_time = time.time()\n",
        "\n",
        "jaccard_gpu_results = calculate_jaccard_gpu_pytorch(\n",
        "    user_business_list_unique,\n",
        "    sample_size=10000,\n",
        "    batch_size=1000  # Adjust based on GPU memory\n",
        ")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"\\nGPU Jaccard calculation completed in {elapsed_time:.2f} seconds\")\n",
        "print(f\"Found {len(jaccard_gpu_results)} user-user similarity pairs\")\n",
        "print(\"\\nTop 10 most similar user pairs (GPU Jaccard):\")\n",
        "print(jaccard_gpu_results.nlargest(10, 'similarity'))\n",
        "\n",
        "# Compare to CPU time estimate\n",
        "estimated_cpu_hours = 10\n",
        "speedup = (estimated_cpu_hours * 3600) / elapsed_time\n",
        "print(f\"\\nEstimated speedup: {speedup:.0f}x faster than CPU\")"
      ],
      "metadata": {
        "id": "KOyC56Xz2c-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: GPU Matrix Factorization for User Similarity\n",
        "# Using GPU-accelerated SVD for collaborative filtering\n",
        "# Reference: Koren et al. (2009) Matrix Factorisation Techniques for Recommender Systems\n",
        "\n",
        "import torch\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def matrix_factorization_similarity_gpu(user_business_list_unique, n_factors=50, sample_size=10000):\n",
        "    \"\"\"\n",
        "    Calculate user similarity using GPU-accelerated matrix factorisation (SVD).\n",
        "\n",
        "    This method reduces the dimensionality of the user-item matrix\n",
        "    to find latent factors that explain user preferences.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    user_business_list_unique : list\n",
        "        List of (user_id, business_list) tuples\n",
        "    n_factors : int\n",
        "        Number of latent factors to extract\n",
        "    sample_size : int\n",
        "        Maximum number of users to process\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame : User similarity scores\n",
        "\n",
        "    References:\n",
        "    -----------\n",
        "    Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorisation techniques\n",
        "    for recommender systems. Computer, 42(8), 30-37.\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device} for matrix factorisation\")\n",
        "\n",
        "    # Convert to dictionary format\n",
        "    user_business_dict = {}\n",
        "    for user_id, business_list in user_business_list_unique:\n",
        "        user_business_dict[user_id] = set(business_list)\n",
        "\n",
        "    # Sample users if needed\n",
        "    user_ids = list(user_business_dict.keys())\n",
        "    if len(user_ids) > sample_size:\n",
        "        np.random.seed(42)\n",
        "        user_ids = list(np.random.choice(user_ids, sample_size, replace=False))\n",
        "\n",
        "    print(f\"Processing {len(user_ids)} users for matrix factorisation...\")\n",
        "\n",
        "    # Create business vocabulary\n",
        "    all_businesses = set()\n",
        "    for uid in user_ids:\n",
        "        all_businesses.update(user_business_dict[uid])\n",
        "\n",
        "    business_to_idx = {b: i for i, b in enumerate(sorted(all_businesses))}\n",
        "    n_users = len(user_ids)\n",
        "    n_businesses = len(all_businesses)\n",
        "\n",
        "    print(f\"Matrix dimensions: {n_users} users √ó {n_businesses} businesses\")\n",
        "\n",
        "    # Build user-business matrix on GPU\n",
        "    if device.type == 'cuda':\n",
        "        # GPU implementation using PyTorch\n",
        "        user_business_matrix = torch.zeros((n_users, n_businesses),\n",
        "                                         dtype=torch.float32, device=device)\n",
        "\n",
        "        for i, user_id in enumerate(user_ids):\n",
        "            businesses = user_business_dict[user_id]\n",
        "            business_indices = [business_to_idx[b] for b in businesses if b in business_to_idx]\n",
        "            if business_indices:\n",
        "                user_business_matrix[i, business_indices] = 1.0\n",
        "\n",
        "        # GPU-accelerated SVD using PyTorch\n",
        "        print(\"Applying GPU-accelerated SVD...\")\n",
        "        U, S, V = torch.svd_lowrank(user_business_matrix, q=n_factors)\n",
        "\n",
        "        # Compute user embeddings\n",
        "        user_embeddings = U @ torch.diag(torch.sqrt(S))\n",
        "\n",
        "        # Calculate cosine similarity on GPU\n",
        "        print(\"Computing cosine similarity on GPU...\")\n",
        "        # Normalise embeddings\n",
        "        user_embeddings_norm = user_embeddings / torch.norm(user_embeddings, dim=1, keepdim=True)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        similarity_matrix = torch.mm(user_embeddings_norm, user_embeddings_norm.t())\n",
        "\n",
        "        # Move to CPU for extraction\n",
        "        similarity_matrix = similarity_matrix.cpu().numpy()\n",
        "\n",
        "    else:\n",
        "        # CPU fallback using scikit-learn\n",
        "        from scipy.sparse import csr_matrix\n",
        "\n",
        "        rows, cols = [], []\n",
        "        for i, user_id in enumerate(user_ids):\n",
        "            businesses = user_business_dict[user_id]\n",
        "            for business in businesses:\n",
        "                if business in business_to_idx:\n",
        "                    rows.append(i)\n",
        "                    cols.append(business_to_idx[business])\n",
        "\n",
        "        data = np.ones(len(rows))\n",
        "        user_item_matrix = csr_matrix((data, (rows, cols)),\n",
        "                                      shape=(n_users, n_businesses))\n",
        "\n",
        "        # Apply SVD\n",
        "        svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
        "        user_embeddings = svd.fit_transform(user_item_matrix)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity_matrix = sklearn_cosine_similarity(user_embeddings)\n",
        "\n",
        "    # Extract top similar users\n",
        "    print(\"Extracting similarity pairs...\")\n",
        "    mf_results = []\n",
        "\n",
        "    for i in range(n_users):\n",
        "        # Get top 10 similar users (excluding self)\n",
        "        top_indices = np.argsort(similarity_matrix[i])[::-1][1:11]\n",
        "\n",
        "        for j in top_indices:\n",
        "            if j < n_users and similarity_matrix[i, j] > 0:\n",
        "                mf_results.append({\n",
        "                    'user_a': user_ids[i],\n",
        "                    'user_b': user_ids[j],\n",
        "                    'similarity': float(similarity_matrix[i, j])\n",
        "                })\n",
        "\n",
        "    # Clear GPU memory if used\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return pd.DataFrame(mf_results)\n",
        "\n",
        "# Apply matrix factorisation\n",
        "print(\"\\nStarting GPU Matrix Factorisation...\")\n",
        "start_time = time.time()\n",
        "\n",
        "mf_results_df = matrix_factorization_similarity_gpu(\n",
        "    user_business_list_unique,\n",
        "    n_factors=50,\n",
        "    sample_size=10000\n",
        ")\n",
        "\n",
        "mf_elapsed = time.time() - start_time\n",
        "print(f\"Matrix Factorisation completed in {mf_elapsed:.2f} seconds\")\n",
        "print(f\"Found {len(mf_results_df)} similarity pairs\")\n",
        "print(\"\\nTop 10 most similar user pairs (Matrix Factorisation):\")\n",
        "print(mf_results_df.nlargest(10, 'similarity'))"
      ],
      "metadata": {
        "id": "RBHR9pXi6nA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4b: Alternative Pure Python MinHash Implementation\n",
        "# Use this if datasketch installation fails\n",
        "\n",
        "import hashlib\n",
        "import random\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SimpleMinHash:\n",
        "    \"\"\"\n",
        "    Simple MinHash implementation without external dependencies.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_perm=128, seed=42):\n",
        "        self.num_perm = num_perm\n",
        "        self.seed = seed\n",
        "        random.seed(seed)\n",
        "\n",
        "        # Generate hash functions (using different seeds)\n",
        "        self.hash_funcs = []\n",
        "        for i in range(num_perm):\n",
        "            a = random.randint(1, 2**32 - 1)\n",
        "            b = random.randint(0, 2**32 - 1)\n",
        "            self.hash_funcs.append((a, b))\n",
        "\n",
        "        self.minhashes = [float('inf')] * num_perm\n",
        "\n",
        "    def update(self, item):\n",
        "        \"\"\"Add an item to the MinHash.\"\"\"\n",
        "        item_hash = int(hashlib.md5(item).hexdigest(), 16)\n",
        "\n",
        "        for i, (a, b) in enumerate(self.hash_funcs):\n",
        "            # Simple hash function: (a * x + b) mod large_prime\n",
        "            hash_val = (a * item_hash + b) % (2**32 - 1)\n",
        "            if hash_val < self.minhashes[i]:\n",
        "                self.minhashes[i] = hash_val\n",
        "\n",
        "    def jaccard(self, other):\n",
        "        \"\"\"Estimate Jaccard similarity with another MinHash.\"\"\"\n",
        "        if len(self.minhashes) != len(other.minhashes):\n",
        "            raise ValueError(\"MinHash signatures must have the same length\")\n",
        "\n",
        "        matches = sum(1 for a, b in zip(self.minhashes, other.minhashes) if a == b)\n",
        "        return matches / len(self.minhashes)\n",
        "\n",
        "def simple_minhash_similarity(user_business_list_unique,\n",
        "                            num_perm=128,\n",
        "                            sample_size=10000,\n",
        "                            min_similarity=0.3):\n",
        "    \"\"\"\n",
        "    Calculate approximate Jaccard similarity using simple MinHash.\n",
        "\n",
        "    This is a fallback implementation that doesn't require external packages.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to dictionary format\n",
        "    user_business_dict = {}\n",
        "    for user_id, business_list in user_business_list_unique:\n",
        "        user_business_dict[user_id] = set(business_list)\n",
        "\n",
        "    # Sample users if needed\n",
        "    user_ids = list(user_business_dict.keys())\n",
        "    if len(user_ids) > sample_size:\n",
        "        print(f\"Sampling {sample_size} users from {len(user_ids)} total users...\")\n",
        "        np.random.seed(42)\n",
        "        sampled_ids = np.random.choice(user_ids, sample_size, replace=False)\n",
        "        sampled_dict = {uid: user_business_dict[uid] for uid in sampled_ids}\n",
        "    else:\n",
        "        sampled_dict = user_business_dict\n",
        "\n",
        "    # Create MinHash for each user\n",
        "    user_minhashes = {}\n",
        "\n",
        "    print(f\"Creating MinHash signatures for {len(sampled_dict)} users...\")\n",
        "\n",
        "    for user_id, businesses in tqdm(sampled_dict.items()):\n",
        "        if len(businesses) == 0:\n",
        "            continue\n",
        "\n",
        "        mh = SimpleMinHash(num_perm=num_perm)\n",
        "        for business in businesses:\n",
        "            mh.update(str(business).encode('utf8'))\n",
        "\n",
        "        user_minhashes[user_id] = mh\n",
        "\n",
        "    # Find similar pairs\n",
        "    similar_pairs = []\n",
        "    user_list = list(user_minhashes.keys())\n",
        "\n",
        "    print(\"Computing pairwise similarities...\")\n",
        "\n",
        "    # For efficiency, only compute upper triangle of similarity matrix\n",
        "    total_pairs = len(user_list) * (len(user_list) - 1) // 2\n",
        "\n",
        "    with tqdm(total=total_pairs) as pbar:\n",
        "        for i in range(len(user_list)):\n",
        "            for j in range(i + 1, len(user_list)):\n",
        "                user_a = user_list[i]\n",
        "                user_b = user_list[j]\n",
        "\n",
        "                similarity = user_minhashes[user_a].jaccard(user_minhashes[user_b])\n",
        "\n",
        "                if similarity >= min_similarity:\n",
        "                    similar_pairs.append({\n",
        "                        'user_a': user_a,\n",
        "                        'user_b': user_b,\n",
        "                        'similarity': similarity\n",
        "                    })\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "    return pd.DataFrame(similar_pairs)\n",
        "\n",
        "# Run the simple MinHash implementation\n",
        "print(\"\\nUsing Pure Python MinHash Implementation...\")\n",
        "start_time = time.time()\n",
        "\n",
        "minhash_df = simple_minhash_similarity(\n",
        "    user_business_list_unique,\n",
        "    num_perm=128,\n",
        "    sample_size=2000,  # Smaller sample for pure Python implementation\n",
        "    min_similarity=0.3\n",
        ")\n",
        "\n",
        "minhash_elapsed = time.time() - start_time\n",
        "print(f\"MinHash calculation completed in {minhash_elapsed:.2f} seconds\")\n",
        "print(f\"Found {len(minhash_df)} similar user pairs\")\n",
        "\n",
        "if len(minhash_df) > 0:\n",
        "    print(\"\\nTop 10 most similar user pairs (MinHash):\")\n",
        "    print(minhash_df.nlargest(10, 'similarity'))\n",
        "else:\n",
        "    print(\"No similar pairs found with the given threshold\")"
      ],
      "metadata": {
        "id": "aA5RiLUj7oNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: GPU-Accelerated Graph-based Similarity\n",
        "# Using PyTorch for GPU graph processing\n",
        "# Reference: Newman (2010) Networks: An Introduction\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def graph_based_similarity_gpu(user_business_list_unique,\n",
        "                              embedding_dim=64,\n",
        "                              sample_size=10000,\n",
        "                              use_gpu=True,\n",
        "                              learning_rate=0.01,\n",
        "                              n_epochs=100,\n",
        "                              batch_size=1024):\n",
        "    \"\"\"\n",
        "    Calculate user similarity using graph embeddings with GPU acceleration.\n",
        "\n",
        "    This method creates a bipartite graph of users and businesses,\n",
        "    then learns embeddings using a simplified DeepWalk approach.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    user_business_list_unique : list\n",
        "        List of (user_id, business_list) tuples\n",
        "    embedding_dim : int\n",
        "        Dimension of node embeddings\n",
        "    sample_size : int\n",
        "        Maximum number of users to process\n",
        "    use_gpu : bool\n",
        "        Whether to use GPU acceleration\n",
        "    learning_rate : float\n",
        "        Learning rate for optimisation\n",
        "    n_epochs : int\n",
        "        Number of training epochs\n",
        "    batch_size : int\n",
        "        Batch size for training\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame : User similarity scores\n",
        "\n",
        "    References:\n",
        "    -----------\n",
        "    Grover, A., & Leskovec, J. (2016). node2vec: Scalable feature learning\n",
        "    for networks. Proceedings of KDD, 855-864.\n",
        "\n",
        "    Newman, M. (2010). Networks: An Introduction. Oxford University Press.\n",
        "\n",
        "    Perozzi, B., Al-Rfou, R., & Skiena, S. (2014). DeepWalk: Online learning\n",
        "    of social representations. Proceedings of KDD, 701-710.\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
        "    print(f\"Using device: {device} for graph embeddings\")\n",
        "\n",
        "    # Convert to dictionary format and sample\n",
        "    user_business_dict = {}\n",
        "    for user_id, business_list in user_business_list_unique:\n",
        "        user_business_dict[user_id] = set(business_list)\n",
        "\n",
        "    user_ids = list(user_business_dict.keys())\n",
        "    if len(user_ids) > sample_size:\n",
        "        np.random.seed(42)\n",
        "        user_ids = list(np.random.choice(user_ids, sample_size, replace=False))\n",
        "        user_business_dict = {uid: user_business_dict[uid] for uid in user_ids}\n",
        "\n",
        "    print(f\"Building bipartite graph with {len(user_ids)} users...\")\n",
        "\n",
        "    # Build node mappings\n",
        "    all_businesses = set()\n",
        "    for businesses in user_business_dict.values():\n",
        "        all_businesses.update(businesses)\n",
        "\n",
        "    # Create node indices\n",
        "    user_to_idx = {uid: i for i, uid in enumerate(user_ids)}\n",
        "    business_to_idx = {bid: i + len(user_ids)\n",
        "                      for i, bid in enumerate(all_businesses)}\n",
        "\n",
        "    n_users = len(user_ids)\n",
        "    n_businesses = len(all_businesses)\n",
        "    n_nodes = n_users + n_businesses\n",
        "\n",
        "    print(f\"Total nodes: {n_nodes} ({n_users} users, {n_businesses} businesses)\")\n",
        "\n",
        "    # Build edge list more efficiently\n",
        "    edges = []\n",
        "    edge_weights = []\n",
        "\n",
        "    for user_id, businesses in user_business_dict.items():\n",
        "        user_idx = user_to_idx[user_id]\n",
        "        for business in businesses:\n",
        "            business_idx = business_to_idx[business]\n",
        "            # Add both directions for undirected graph\n",
        "            edges.append([user_idx, business_idx])\n",
        "            edges.append([business_idx, user_idx])\n",
        "            edge_weights.extend([1.0, 1.0])\n",
        "\n",
        "    print(f\"Total edges: {len(edges)}\")\n",
        "\n",
        "    # Convert to tensors\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().to(device)\n",
        "    edge_weight = torch.tensor(edge_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Define embedding model\n",
        "    class GraphEmbedding(torch.nn.Module):\n",
        "        def __init__(self, n_nodes, embedding_dim):\n",
        "            super().__init__()\n",
        "            # Initialise embeddings with Xavier/Glorot initialisation\n",
        "            self.embeddings = torch.nn.Parameter(\n",
        "                torch.nn.init.xavier_uniform_(torch.empty(n_nodes, embedding_dim))\n",
        "            )\n",
        "\n",
        "        def forward(self, node_indices):\n",
        "            return self.embeddings[node_indices]\n",
        "\n",
        "        def get_all_embeddings(self):\n",
        "            return F.normalize(self.embeddings, p=2, dim=1)\n",
        "\n",
        "    # Initialise model\n",
        "    model = GraphEmbedding(n_nodes, embedding_dim).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training\n",
        "    print(\"Learning graph embeddings on GPU...\")\n",
        "    model.train()\n",
        "\n",
        "    # Create edge batches for efficient training\n",
        "    n_edges = edge_index.shape[1]\n",
        "    edge_indices = torch.arange(n_edges, device=device)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        # Shuffle edges\n",
        "        perm = torch.randperm(n_edges, device=device)\n",
        "\n",
        "        # Process in batches\n",
        "        for i in range(0, n_edges, batch_size):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get batch\n",
        "            batch_indices = perm[i:i+batch_size]\n",
        "            batch_edges = edge_index[:, batch_indices]\n",
        "\n",
        "            # Positive samples\n",
        "            src = batch_edges[0]\n",
        "            dst = batch_edges[1]\n",
        "\n",
        "            # Get embeddings\n",
        "            src_emb = model(src)\n",
        "            dst_emb = model(dst)\n",
        "\n",
        "            # Negative sampling - sample random destinations\n",
        "            neg_dst = torch.randint(0, n_nodes, (len(src),), device=device)\n",
        "            neg_emb = model(neg_dst)\n",
        "\n",
        "            # Compute scores\n",
        "            pos_scores = (src_emb * dst_emb).sum(dim=1)\n",
        "            neg_scores = (src_emb * neg_emb).sum(dim=1)\n",
        "\n",
        "            # Binary cross-entropy loss\n",
        "            pos_loss = F.binary_cross_entropy_with_logits(\n",
        "                pos_scores, torch.ones_like(pos_scores)\n",
        "            )\n",
        "            neg_loss = F.binary_cross_entropy_with_logits(\n",
        "                neg_scores, torch.zeros_like(neg_scores)\n",
        "            )\n",
        "\n",
        "            loss = pos_loss + neg_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 20 == 0:\n",
        "            avg_loss = epoch_loss / (n_edges // batch_size)\n",
        "            print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Get final embeddings\n",
        "    print(\"Computing user similarities...\")\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        all_embeddings = model.get_all_embeddings()\n",
        "        user_embeddings = all_embeddings[:n_users].cpu().numpy()\n",
        "\n",
        "    # Calculate cosine similarity between users\n",
        "    # The embeddings are already normalised, so dot product gives cosine similarity\n",
        "    similarity_matrix = np.dot(user_embeddings, user_embeddings.T)\n",
        "\n",
        "    # Extract similarity pairs (avoiding duplicates)\n",
        "    results = []\n",
        "\n",
        "    # Only process upper triangle of similarity matrix\n",
        "    print(\"Extracting similarity pairs...\")\n",
        "\n",
        "    for i in tqdm(range(n_users)):\n",
        "        # Get top similar users (excluding self)\n",
        "        similarities = similarity_matrix[i]\n",
        "        # Set self-similarity to -inf to exclude it\n",
        "        similarities[i] = -np.inf\n",
        "\n",
        "        # Get top 10 similar users\n",
        "        top_indices = np.argsort(similarities)[::-1][:10]\n",
        "\n",
        "        for j in top_indices:\n",
        "            if similarities[j] > 0:  # Only include positive similarities\n",
        "                # Only add if this is an upper triangle entry (avoid duplicates)\n",
        "                if i < j:\n",
        "                    results.append({\n",
        "                        'user_a': user_ids[i],\n",
        "                        'user_b': user_ids[j],\n",
        "                        'similarity': float(similarities[j])\n",
        "                    })\n",
        "\n",
        "    # Clear GPU memory\n",
        "    del model, edge_index, edge_weight, all_embeddings\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Alternative simpler implementation if the above fails\n",
        "def simple_graph_similarity(user_business_list_unique, sample_size=5000):\n",
        "    \"\"\"\n",
        "    Simpler graph-based similarity using shared businesses.\n",
        "    Falls back to CPU-based computation.\n",
        "    \"\"\"\n",
        "    print(\"Using simplified graph similarity approach...\")\n",
        "\n",
        "    # Convert to dictionary\n",
        "    user_business_dict = {}\n",
        "    for user_id, business_list in user_business_list_unique:\n",
        "        user_business_dict[user_id] = set(business_list)\n",
        "\n",
        "    # Sample users\n",
        "    user_ids = list(user_business_dict.keys())\n",
        "    if len(user_ids) > sample_size:\n",
        "        np.random.seed(42)\n",
        "        user_ids = list(np.random.choice(user_ids, sample_size, replace=False))\n",
        "\n",
        "    # Build business-to-users mapping\n",
        "    business_to_users = {}\n",
        "    for user_id in user_ids:\n",
        "        for business in user_business_dict[user_id]:\n",
        "            if business not in business_to_users:\n",
        "                business_to_users[business] = set()\n",
        "            business_to_users[business].add(user_id)\n",
        "\n",
        "    # Calculate similarities based on common businesses\n",
        "    results = []\n",
        "\n",
        "    print(\"Computing similarities based on shared businesses...\")\n",
        "    for i, user_a in enumerate(tqdm(user_ids)):\n",
        "        businesses_a = user_business_dict[user_a]\n",
        "\n",
        "        # Find potentially similar users (those who share businesses)\n",
        "        candidate_users = set()\n",
        "        for business in businesses_a:\n",
        "            if business in business_to_users:\n",
        "                candidate_users.update(business_to_users[business])\n",
        "        candidate_users.discard(user_a)\n",
        "\n",
        "        # Calculate similarity with candidates\n",
        "        similarities = []\n",
        "        for user_b in candidate_users:\n",
        "            if user_b in user_business_dict:\n",
        "                businesses_b = user_business_dict[user_b]\n",
        "\n",
        "                # Jaccard similarity\n",
        "                intersection = len(businesses_a & businesses_b)\n",
        "                union = len(businesses_a | businesses_b)\n",
        "\n",
        "                if union > 0:\n",
        "                    similarity = intersection / union\n",
        "                    similarities.append((user_b, similarity))\n",
        "\n",
        "        # Get top 10\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        for user_b, sim in similarities[:10]:\n",
        "            if sim > 0:\n",
        "                # Only add if this avoids duplicates\n",
        "                if user_a < user_b:\n",
        "                    results.append({\n",
        "                        'user_a': user_a,\n",
        "                        'user_b': user_b,\n",
        "                        'similarity': sim\n",
        "                    })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Try the improved graph-based similarity\n",
        "print(\"\\nStarting GPU Graph-based similarity...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    graph_results_df = graph_based_similarity_gpu(\n",
        "        user_business_list_unique,\n",
        "        embedding_dim=64,\n",
        "        sample_size=5000,\n",
        "        learning_rate=0.01,\n",
        "        n_epochs=50,  # Reduced epochs for faster execution\n",
        "        batch_size=1024\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå GPU graph method failed: {str(e)}\")\n",
        "    print(\"Falling back to simple graph similarity...\")\n",
        "\n",
        "    graph_results_df = simple_graph_similarity(\n",
        "        user_business_list_unique,\n",
        "        sample_size=5000\n",
        "    )\n",
        "\n",
        "graph_elapsed = time.time() - start_time\n",
        "print(f\"\\nGraph-based similarity completed in {graph_elapsed:.2f} seconds\")\n",
        "print(f\"Found {len(graph_results_df)} similarity pairs\")\n",
        "\n",
        "if len(graph_results_df) > 0:\n",
        "    print(\"\\nTop 10 most similar user pairs (Graph-based):\")\n",
        "    print(graph_results_df.nlargest(10, 'similarity'))\n",
        "\n",
        "    print(\"\\nSimilarity distribution:\")\n",
        "    print(graph_results_df['similarity'].describe())\n",
        "else:\n",
        "    print(\"No similarity pairs found\")"
      ],
      "metadata": {
        "id": "WPM8urIo8eXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 ‚Äî Compare and evaluate similarity methods (clean, annotated)\n",
        "# References:\n",
        "# - Herlocker, J.L., Konstan, J.A., Terveen, L.G. and Riedl, J.T. (2004) Evaluating collaborative filtering recommender systems,\n",
        "#   ACM Transactions on Information Systems, 22(1), 5‚Äì53. We follow their guidance on method comparison and diagnostics.\n",
        "# - Jaccard index definition used for agreement between method outputs: intersection over union of top-N pairs.\n",
        "# - We use pandas.DataFrame.nlargest for efficient top-N selection; matplotlib/seaborn for bar charts and heatmaps.\n",
        "#\n",
        "# Citations:\n",
        "# Herlocker et al. (2004): see PDF. [Link in references]\n",
        "# pandas nlargest: see docs. [Link in references]\n",
        "# matplotlib bar: see docs. [Link in references]\n",
        "# seaborn heatmap: see docs. [Link in references]\n",
        "# Jaccard index: see overview. [Link in references]\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def _coerce_result_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Internal helper: ensure the result DataFrame has the required columns and numeric similarity.\n",
        "    Expected columns: ['user_a', 'user_b', 'similarity'].\n",
        "    Returns a sanitized copy or an empty DataFrame if validation fails.\n",
        "    \"\"\"\n",
        "    if df is None or not isinstance(df, pd.DataFrame) or df.empty:\n",
        "        return pd.DataFrame(columns=[\"user_a\", \"user_b\", \"similarity\"])\n",
        "    df = df.copy()\n",
        "    # Allow a few alternate column names and normalize them.\n",
        "    rename_map = {}\n",
        "    if \"src\" in df.columns and \"user_a\" not in df.columns:\n",
        "        rename_map[\"src\"] = \"user_a\"\n",
        "    if \"dst\" in df.columns and \"user_b\" not in df.columns:\n",
        "        rename_map[\"dst\"] = \"user_b\"\n",
        "    if \"score\" in df.columns and \"similarity\" not in df.columns:\n",
        "        rename_map[\"score\"] = \"similarity\"\n",
        "    df = df.rename(columns=rename_map)\n",
        "\n",
        "    required = {\"user_a\", \"user_b\", \"similarity\"}\n",
        "    if not required.issubset(df.columns):\n",
        "        return pd.DataFrame(columns=[\"user_a\", \"user_b\", \"similarity\"])\n",
        "\n",
        "    # Coerce similarity to numeric and drop rows without valid values.\n",
        "    df[\"similarity\"] = pd.to_numeric(df[\"similarity\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"user_a\", \"user_b\", \"similarity\"])\n",
        "    # Standardize pair ordering for reproducibility in overlaps.\n",
        "    # Convert to strings to avoid accidental dtype issues in tuple sorting.\n",
        "    df[\"user_a\"] = df[\"user_a\"].astype(str)\n",
        "    df[\"user_b\"] = df[\"user_b\"].astype(str)\n",
        "    df[\"pair\"] = df.apply(lambda r: tuple(sorted([r[\"user_a\"], r[\"user_b\"]])), axis=1)\n",
        "    return df[[\"user_a\", \"user_b\", \"pair\", \"similarity\"]]\n",
        "\n",
        "def compare_similarity_methods(\n",
        "    method_frames: dict,\n",
        "    method_times: dict = None,\n",
        "    top_n: int = 100\n",
        "):\n",
        "    \"\"\"\n",
        "    Compare multiple similarity result sets and report:\n",
        "      1) Summary table per method (count, mean/std/min/max/quantiles, elapsed time).\n",
        "      2) Overlap matrix (size of intersection among top-N pairs).\n",
        "      3) Jaccard-agreement matrix for top-N sets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    method_frames : dict\n",
        "        Mapping of method name -> pandas DataFrame with columns ['user_a', 'user_b', 'similarity'].\n",
        "    method_times : dict, optional\n",
        "        Mapping of method name -> elapsed time in seconds (float). Missing names default to NaN.\n",
        "    top_n : int\n",
        "        Number of top pairs (by similarity) to use for overlap/Jaccard comparisons.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    comparison_df : pd.DataFrame\n",
        "        Summary metrics per method, sorted by Time (seconds).\n",
        "    overlap_matrix : pd.DataFrame\n",
        "        Pairwise counts of common pairs among top-N sets.\n",
        "    jaccard_matrix : pd.DataFrame\n",
        "        Pairwise Jaccard similarities among top-N sets.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - We use top-N set overlap and Jaccard agreement to compare method outputs in line with evaluation\n",
        "      guidance discussed in Herlocker et al. (2004).\n",
        "    - Top-N selection uses pandas.DataFrame.nlargest for performance.\n",
        "    \"\"\"\n",
        "    method_times = method_times or {}\n",
        "\n",
        "    # 1) Sanitize and filter method frames\n",
        "    cleaned = {}\n",
        "    for name, df in method_frames.items():\n",
        "        df_clean = _coerce_result_df(df)\n",
        "        if not df_clean.empty:\n",
        "            cleaned[name] = df_clean\n",
        "\n",
        "    # 2) Build summary comparison table\n",
        "    rows = []\n",
        "    for name, df in cleaned.items():\n",
        "        s = df[\"similarity\"]\n",
        "        rows.append({\n",
        "            \"Method\": name,\n",
        "            \"Pairs Found\": len(df),\n",
        "            \"Avg Similarity\": float(s.mean()),\n",
        "            \"Std Similarity\": float(s.std(ddof=1)) if len(s) > 1 else 0.0,\n",
        "            \"Min Similarity\": float(s.min()),\n",
        "            \"Max Similarity\": float(s.max()),\n",
        "            \"Q25 Similarity\": float(s.quantile(0.25)),\n",
        "            \"Q50 Similarity\": float(s.quantile(0.50)),\n",
        "            \"Q75 Similarity\": float(s.quantile(0.75)),\n",
        "            \"Time (seconds)\": float(method_times.get(name, np.nan))\n",
        "        })\n",
        "    comparison_df = pd.DataFrame(rows)\n",
        "    comparison_df = comparison_df.sort_values(by=\"Time (seconds)\", na_position=\"last\").reset_index(drop=True)\n",
        "\n",
        "    # 3) Top-N sets per method (by similarity)\n",
        "    top_sets = {}\n",
        "    for name, df in cleaned.items():\n",
        "        # nlargest is recommended for top-N efficiency (pandas docs).\n",
        "        top_df = df.nlargest(min(top_n, len(df)), columns=\"similarity\")\n",
        "        top_sets[name] = set(top_df[\"pair\"])\n",
        "\n",
        "    # 4) Overlap matrix (cardinality of intersections)\n",
        "    names = list(top_sets.keys())\n",
        "    overlap_matrix = pd.DataFrame(index=names, columns=names, dtype=float)\n",
        "    for i, a in enumerate(names):\n",
        "        for j, b in enumerate(names):\n",
        "            if a == b:\n",
        "                overlap_matrix.iloc[i, j] = float(len(top_sets[a]))\n",
        "            else:\n",
        "                overlap_matrix.iloc[i, j] = float(len(top_sets[a] & top_sets[b]))\n",
        "\n",
        "    # 5) Jaccard agreement matrix among top-N sets\n",
        "    jaccard_matrix = pd.DataFrame(index=names, columns=names, dtype=float)\n",
        "    for i, a in enumerate(names):\n",
        "        for j, b in enumerate(names):\n",
        "            if a == b:\n",
        "                jaccard_matrix.iloc[i, j] = 1.0\n",
        "            else:\n",
        "                inter = float(len(top_sets[a] & top_sets[b]))\n",
        "                union = float(len(top_sets[a] | top_sets[b]))\n",
        "                jaccard_matrix.iloc[i, j] = (inter / union) if union > 0 else 0.0\n",
        "\n",
        "    # 6) Print and plot\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SIMILARITY METHOD COMPARISON\")\n",
        "    print(\"=\" * 80)\n",
        "    if not comparison_df.empty:\n",
        "        print(comparison_df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"No valid methods to compare.\")\n",
        "\n",
        "    # Execution time bar chart\n",
        "    if not comparison_df.empty and comparison_df[\"Time (seconds)\"].notna().any():\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.bar(comparison_df[\"Method\"], comparison_df[\"Time (seconds)\"])\n",
        "        plt.xlabel(\"Method\")\n",
        "        plt.ylabel(\"Execution Time (seconds)\")\n",
        "        plt.title(\"Similarity Method Performance Comparison\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Histograms of similarity distributions\n",
        "    if cleaned:\n",
        "        cols = 3\n",
        "        rows_plt = int(np.ceil(len(cleaned) / cols))\n",
        "        fig, axes = plt.subplots(rows_plt, cols, figsize=(5 * cols, 4 * rows_plt))\n",
        "        axes = np.atleast_1d(axes).ravel()\n",
        "        for ax in axes[len(cleaned):]:\n",
        "            ax.remove()\n",
        "        for ax, (name, df) in zip(axes, cleaned.items()):\n",
        "            ax.hist(df[\"similarity\"], bins=50, edgecolor=\"black\")\n",
        "            mean_val = df[\"similarity\"].mean()\n",
        "            median_val = df[\"similarity\"].median()\n",
        "            ax.axvline(mean_val, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Mean: {mean_val:.3f}\")\n",
        "            ax.axvline(median_val, color=\"green\", linestyle=\"--\", linewidth=2, label=f\"Median: {median_val:.3f}\")\n",
        "            ax.set_title(f\"{name} Similarity Distribution\")\n",
        "            ax.set_xlabel(\"Similarity Score\")\n",
        "            ax.set_ylabel(\"Frequency\")\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Overlap heatmap\n",
        "    if not overlap_matrix.empty:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(overlap_matrix.astype(float), annot=True, fmt=\".0f\", cmap=\"YlOrRd\",\n",
        "                    cbar_kws={\"label\": \"Number of Common Pairs\"})\n",
        "        plt.title(f\"Overlap Between Top {top_n} User Pairs Across Methods\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Jaccard-agreement heatmap\n",
        "    if not jaccard_matrix.empty:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(jaccard_matrix.astype(float), annot=True, fmt=\".3f\", cmap=\"viridis\",\n",
        "                    vmin=0, vmax=1, cbar_kws={\"label\": \"Jaccard Similarity\"})\n",
        "        plt.title(f\"Agreement Between Methods (Jaccard of Top {top_n} Pairs)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Simple performance vs average-similarity scatter\n",
        "    if not comparison_df.empty:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        x = comparison_df[\"Time (seconds)\"].fillna(comparison_df[\"Time (seconds)\"].max())\n",
        "        y = comparison_df[\"Avg Similarity\"]\n",
        "        sizes = comparison_df[\"Pairs Found\"] / 100.0\n",
        "        plt.scatter(x, y, s=sizes, alpha=0.7)\n",
        "        for _, r in comparison_df.iterrows():\n",
        "            plt.annotate(r[\"Method\"], (r[\"Time (seconds)\"] if not np.isnan(r[\"Time (seconds)\"]) else x.max(), r[\"Avg Similarity\"]),\n",
        "                         xytext=(5, 5), textcoords=\"offset points\", fontsize=9)\n",
        "        plt.xlabel(\"Execution Time (seconds)\")\n",
        "        plt.ylabel(\"Average Similarity Score\")\n",
        "        plt.title(\"Performance vs Average Similarity (bubble size = number of pairs / 100)\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Summary insights\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SUMMARY INSIGHTS\")\n",
        "    print(\"=\" * 80)\n",
        "    if not comparison_df.empty:\n",
        "        # Fastest\n",
        "        if comparison_df[\"Time (seconds)\"].notna().any():\n",
        "            fastest = comparison_df.loc[comparison_df[\"Time (seconds)\"].idxmin()]\n",
        "            print(f\"Fastest method: {fastest['Method']} ({fastest['Time (seconds)']:.2f} seconds)\")\n",
        "        # Most pairs\n",
        "        most_pairs = comparison_df.loc[comparison_df[\"Pairs Found\"].idxmax()]\n",
        "        print(f\"Most pairs found: {most_pairs['Method']} ({int(most_pairs['Pairs Found'])} pairs)\")\n",
        "        # Highest average similarity\n",
        "        highest_sim = comparison_df.loc[comparison_df[\"Avg Similarity\"].idxmax()]\n",
        "        print(f\"Highest average similarity: {highest_sim['Method']} ({highest_sim['Avg Similarity']:.4f})\")\n",
        "\n",
        "        # Highest agreement off-diagonal in jaccard_matrix\n",
        "        if not jaccard_matrix.empty and len(jaccard_matrix) > 1:\n",
        "            jacc = jaccard_matrix.to_numpy().astype(float)\n",
        "            mask = ~np.eye(len(jacc), dtype=bool)\n",
        "            if mask.any():\n",
        "                max_idx = np.nanargmax(jacc[mask])\n",
        "                # map back to (row, col)\n",
        "                idxs = np.argwhere(mask)\n",
        "                r, c = idxs[max_idx]\n",
        "                print(f\"Highest agreement: {jaccard_matrix.index[r]} and {jaccard_matrix.columns[c]} \"\n",
        "                      f\"(Jaccard: {jaccard_matrix.iloc[r, c]:.3f})\")\n",
        "    else:\n",
        "        print(\"No valid summaries; please check the inputs.\")\n",
        "\n",
        "    return comparison_df, overlap_matrix, jaccard_matrix\n",
        "\n",
        "# Example usage (kept explicit to avoid hidden globals):\n",
        "# Supply whatever results you actually produced. DataFrames must have columns: user_a, user_b, similarity.\n",
        "# For any method you do not have, pass an empty DataFrame.\n",
        "method_frames = {\n",
        "    \"Jaccard (GPU)\": jaccard_gpu_results if \"jaccard_gpu_results\" in globals() else pd.DataFrame(),\n",
        "    \"Cosine (Sparse)\": topN_df if \"topN_df\" in globals() else pd.DataFrame(),\n",
        "    \"Matrix Factorisation (GPU)\": mf_results_df if \"mf_results_df\" in globals() else pd.DataFrame(),\n",
        "    \"MinHash (Approximate)\": minhash_df if \"minhash_df\" in globals() else pd.DataFrame(),\n",
        "    \"Graph-based (GPU)\": graph_results_df if \"graph_results_df\" in globals() else pd.DataFrame(),\n",
        "}\n",
        "method_times = {\n",
        "    # Provide timings if you recorded them; otherwise omit or leave as NaN.\n",
        "    \"Jaccard (GPU)\": jaccard_time if \"jaccard_time\" in globals() else np.nan,\n",
        "    \"Cosine (Sparse)\": cosine_time if \"cosine_time\" in globals() else np.nan,\n",
        "    \"Matrix Factorisation (GPU)\": mf_time if \"mf_time\" in globals() else np.nan,\n",
        "    \"MinHash (Approximate)\": minhash_time if \"minhash_time\" in globals() else np.nan,\n",
        "    \"Graph-based (GPU)\": graph_time if \"graph_time\" in globals() else np.nan,\n",
        "}\n",
        "\n",
        "comparison_df, overlap_matrix, jaccard_matrix = compare_similarity_methods(\n",
        "    method_frames=method_frames,\n",
        "    method_times=method_times,\n",
        "    top_n=100\n",
        ")\n"
      ],
      "metadata": {
        "id": "fSqwUO1eAdY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We choose sparse cosine on the user business incidence matrix because it gives us small, easy-to-understand neighbour sets, works well with sparse algebra, and is consistent with standard user-based CF practice (our run: mean0.25, stable top-N).\n",
        "\n",
        "We tried a few different methods: GPU Jaccard (lots of pairs but low scores), MinHash (fast approximate, few pairs), matrix factorisation with latent-space cosine (highest mean‚âà0.67), and graph embeddings (mean‚âà0.41).\n",
        "\n",
        "There is no one best way to do things; the best way depends on the evaluation goal (accuracy vs. interpretability, scalability, sparsity). Matrix factorisation usually works best for ranking accuracy at scale, but we like sparse cosine for explainable neighbour search and easy maintenance."
      ],
      "metadata": {
        "id": "SpceZo3mG84y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6qzxOHcLG8dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity measures how similar two vectors are by comparing the angle between them, not their magnitude.\n",
        "\n",
        "Formally, for two users A and B represented as vectors of their item interactions:\n",
        "\n",
        "cosine_similarity ( ùê¥ , ùêµ )\n",
        "ùê¥ ‚ãÖ ùêµ ‚à• ùê¥ ‚à• √ó ‚à• ùêµ ‚à• cosine_similarity(A,B)= ‚à•A‚à•√ó‚à•B‚à• A‚ãÖB‚Äã\n",
        "\n",
        "ùê¥ ‚ãÖ ùêµ A‚ãÖB ‚Üí dot product (sum of element-wise products)\n",
        "\n",
        "‚à• ùê¥ ‚à• ‚à•A‚à• and ‚à• ùêµ ‚à• ‚à•B‚à• ‚Üí vector magnitudes (lengths)\n",
        "\n",
        "The result ranges between 0 (completely dissimilar) and 1 (identical directed)"
      ],
      "metadata": {
        "id": "u0_8pBzTvFm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jaccard similarity is a statistic used for comparing the similarity and diversity of sample sets. In this context, we're using it to compare the sets of businesses reviewed by different customers.\n",
        "\n",
        "The strategy used is as follows:\n",
        "\n",
        "Representing Businesses as Sets: For each user, we create a set of the unique businesses they have reviewed using the encoded business names.\n",
        "\n",
        "Calculating Intersection: The intersection of two users' sets includes the businesses that both users have reviewed.\n",
        "\n",
        "Calculating Union: The union of two users' sets includes all unique businesses that either user has reviewed.\n",
        "\n",
        "Jaccard Similarity Formula: The Jaccard similarity is calculated as the size of the intersection divided by the size of the union:\n",
        "\n",
        "Jaccard Similarity (User A, User B) = |Set of businesses reviewed by A ‚à© Set of businesses reviewed by B| / |Set of businesses reviewed by A ‚à™ Set of businesses reviewed by B|\n",
        "\n",
        "A Jaccard similarity of 1 means the two users have reviewed the exact same set of businesses, while a similarity of 0 means they have reviewed completely different sets of businesses. Values in between indicate varying degrees of overlap in their review history.\n",
        "\n",
        "In the code, we are calculating this similarity based on the TF-IDF vectors derived from the encoded business names. The calculate_jaccard_similarity function does this by looking at the indices of the non-zero elements in the sparse vectors, which correspond to the businesses present in the user's review history."
      ],
      "metadata": {
        "id": "vzBSo8ZqiM8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 2**"
      ],
      "metadata": {
        "id": "7Ca7r1oW6oVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages: install and import (simple and annotated)\n",
        "# Why: set up a small, standard toolbox for time series work:\n",
        "# - numpy/pandas/matplotlib/seaborn for data and plots\n",
        "# - statsmodels for classical TS models and diagnostics\n",
        "# - scikit-learn for metrics and simple baselines\n",
        "# - prophet for later structural forecasting\n",
        "# Notes: keep imports lean and human-readable.\n",
        "\n",
        "# Make sure any old \"fbprophet\" is removed and Prophet is available\n",
        "%pip -q install prophet\n",
        "\n",
        "# Core scientific stack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Classical time series (statsmodels)\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# ML utilities\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Prophet (structural forecasting)\n",
        "from prophet import Prophet\n",
        "\n",
        "# Spark helpers (for aggregation in the next cell)\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Plot style\n",
        "%matplotlib inline\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# References:\n",
        "# Hunter, J.D. (2007) Matplotlib: A 2D graphics environment. Computing in Science & Engineering, 9(3), 90‚Äì95.\n",
        "# McKinney, W. (2010) Data structures for statistical computing in Python. In: Proceedings of the 9th Python in Science Conference, pp. 51‚Äì56.\n",
        "# Waskom, M. (2021) seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021.\n",
        "# Seabold, S. and Perktold, J. (2010) statsmodels: Econometric and statistical modeling with Python. Proceedings of the 9th Python in Science Conference.\n",
        "# Pedregosa, F. et al. (2011) Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825‚Äì2830.\n",
        "# Taylor, S.J. and Letham, B. (2018) Forecasting at scale. The American Statistician, 72(1), 37‚Äì45."
      ],
      "metadata": {
        "id": "VwSA1U3L7FTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select dataframe and prepare it for analysis\n",
        "# Task: from joined_df, compute daily review counts and make a clean pandas time series.\n",
        "# Why: most TS methods expect a regular date index with one value per day.\n",
        "# Method: group by date (newtime), count records, convert to pandas, set DateTimeIndex.\n",
        "# Cite: aggregation of counts to a regular TS is standard in classical workflows (Box et al., 2015).\n",
        "\n",
        "# Safety: check joined_df exists\n",
        "try:\n",
        "    _ = joined_df.printSchema()\n",
        "except NameError:\n",
        "    raise RuntimeError(\"joined_df not found. Please run the earlier join cell to create it.\")\n",
        "\n",
        "# 1) Aggregate to reviews per day in Spark\n",
        "daily_counts_spark = (\n",
        "    joined_df\n",
        "    .where(F.col(\"newtime\").isNotNull())\n",
        "    .groupBy(\"newtime\")\n",
        "    .agg(F.count(F.lit(1)).alias(\"reviews\"))\n",
        "    .orderBy(\"newtime\")\n",
        ")\n",
        "\n",
        "# 2) Move to pandas and set a clean daily index\n",
        "daily_pdf = daily_counts_spark.toPandas()\n",
        "daily_pdf[\"newtime\"] = pd.to_datetime(daily_pdf[\"newtime\"])\n",
        "daily_pdf = daily_pdf.sort_values(\"newtime\").set_index(\"newtime\")\n",
        "\n",
        "print(\"First 5 observed days (before any filling):\")\n",
        "display(daily_pdf.head())\n",
        "\n",
        "print(\"\\nQuick shape and date span:\")\n",
        "print(\"rows:\", len(daily_pdf), \"| cols:\", list(daily_pdf.columns))\n",
        "print(\"range:\", daily_pdf.index.min().date(), \"to\", daily_pdf.index.max().date())\n",
        "\n",
        "# References (Harvard style):\n",
        "# Box, G.E.P., Jenkins, G.M., Reinsel, G.C. and Ljung, G.M. (2015) Time Series Analysis: Forecasting and Control. 5th edn. Hoboken, NJ: Wiley."
      ],
      "metadata": {
        "id": "_wca_mmD9AEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 2.1**"
      ],
      "metadata": {
        "id": "pCT7QOgH6rXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Standardize the date column (convert to datetime and normalize to date only)\n",
        "df['review_date'] = pd.to_datetime(df['review_date']).dt.normalize()\n",
        "\n",
        "# Create the daily time series by counting reviews per day\n",
        "daily_reviews = df.groupby('review_date').size().rename('review_count')\n",
        "\n",
        "# Determine the full date range of the dataset\n",
        "min_date = daily_reviews.index.min()\n",
        "max_date = daily_reviews.index.max()\n",
        "full_date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
        "\n",
        "# Reindex the time series to include all days in the range (this introduces NaNs for missing days)\n",
        "full_ts = daily_reviews.reindex(full_date_range)\n",
        "\n",
        "# Calculate the mean review count for imputation: Total Reviews / Total Days in Range\n",
        "total_reviews = daily_reviews.sum()\n",
        "total_days = len(full_date_range)\n",
        "mean_reviews_imputation = total_reviews / total_days\n",
        "\n",
        "# Fill the missing days (NaNs) with the calculated mean\n",
        "imputed_ts = full_ts.fillna(mean_reviews_imputation)\n",
        "imputed_ts.index.freq = 'D' # Set frequency for decomposition\n",
        "\n",
        "print(\"--- Imputation Details ---\")\n",
        "print(f\"Total Reviews: {total_reviews}\")\n",
        "print(f\"Total Days in Full Range: {total_days}\")\n",
        "print(f\"Calculated Mean Reviews per Day (Imputation Value): {mean_reviews_imputation:.2f}\")\n",
        "print(f\"Number of Missing Days Filled: {len(daily_reviews) - len(imputed_ts.dropna())}\")\n",
        "\n",
        "# Decompose the time series using the Additive Model with a weekly period (7 days)\n",
        "decomposition = seasonal_decompose(imputed_ts, model='additive', period=7)\n",
        "\n",
        "#Analysis and Plotting ---\n",
        "fig, axes = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n",
        "decomposition.observed.plot(ax=axes[0], title='Observed Review Submissions (Daily)')\n",
        "decomposition.trend.plot(ax=axes[1], title='Trend Component')\n",
        "decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component (Weekly)')\n",
        "decomposition.resid.plot(ax=axes[3], title='Residual Component')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XJlv9sK658Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 2.2**"
      ],
      "metadata": {
        "id": "1K0CuL1MBHzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the Time Series (80% Train, 20% Test)\n",
        "train_size = int(len(imputed_ts) * 0.8)\n",
        "train, test = imputed_ts[:train_size], imputed_ts[train_size:]\n",
        "print(f\"Training set size: {len(train)}\")\n",
        "print(f\"Testing set size: {len(test)}\")\n",
        "\n",
        "#Define the Grid Search Parameters\n",
        "p_params = d_params = q_params = [0, 1, 2]\n",
        "pdq_combinations = list(product(p_params, d_params, q_params))\n",
        "\n",
        "best_mae = float('inf')\n",
        "best_order = None\n",
        "results = []\n",
        "\n",
        "print(\"\\n--- Starting ARIMA Grid Search (p, d, q in [0, 1, 2]) ---\")\n",
        "\n",
        "#Grid Search Loop\n",
        "for order in pdq_combinations:\n",
        "    try:\n",
        "        # Train the ARIMA Model\n",
        "        model = ARIMA(train, order=order)\n",
        "        model_fit = model.fit()\n",
        "\n",
        "        # Forecast\n",
        "        # Start and end indices for the forecast period (should match the test set)\n",
        "        start_index = len(train)\n",
        "        end_index = len(imputed_ts) - 1\n",
        "        forecast = model_fit.predict(start=start_index, end=end_index)\n",
        "\n",
        "        # Calculate MAE\n",
        "        mae = mean_absolute_error(test, forecast)\n",
        "        results.append({'order': order, 'mae': mae})\n",
        "\n",
        "        # Update Best Model\n",
        "        if mae < best_mae:\n",
        "            best_mae = mae\n",
        "            best_order = order\n",
        "            # Print the new best model as it's found\n",
        "            print(f\"-> New Best: ARIMA{order} - MAE: {mae:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Skip models that fail to converge or raise other errors\n",
        "        continue\n",
        "\n",
        "# Final Output\n",
        "results_df = pd.DataFrame(results).sort_values(by='mae')\n",
        "\n",
        "print(\"\\n--- Grid Search Complete ---\")\n",
        "print(f\"Total models tested: {len(results)}\")\n",
        "print(f\"Best ARIMA Model: ARIMA{best_order} with MAE: {best_mae:.4f}\")\n",
        "print(\"\\nTop 5 Models by MAE:\")\n",
        "display(results_df.head())"
      ],
      "metadata": {
        "id": "jCcX-0Bd6hRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HhM3c1N9BVbN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
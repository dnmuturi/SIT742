{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLSXCum1dzFwqxH2/o562X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnmuturi/SIT742/blob/main/SIT742Task2code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**END TERM ASSIGNMENT**\n",
        "\n",
        " **GROUP C30 Members**\n",
        "\n",
        "1. David Muturi - S225177509\n",
        "2. Nhlanhla Matukane - S225177376\n",
        "3. Vincent Nwobi -"
      ],
      "metadata": {
        "id": "SzkIZK2xplg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 1**"
      ],
      "metadata": {
        "id": "cdkp7fJMbMMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.1**"
      ],
      "metadata": {
        "id": "G0MnsqpJbQ5r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53955062",
        "outputId": "15b5ee12-9fd8-4732-a918-1473723af584"
      },
      "source": [
        "#install the spark library\n",
        "!pip install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load necessary libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, from_unixtime, to_date, when, lit, trim, count, date_format,avg\n",
        "\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "C4Ktnb9OqiWo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Define the path to the CSV file\n",
        "csv_path = '/content/drive/My Drive/SIT742/review.csv'\n",
        "# Define the path to the second CSV file\n",
        "meta_csv_path = '/content/drive/My Drive/SIT742/meta-review-business.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cA9x7Pi0Wc-",
        "outputId": "a5cdd57a-71e6-4194-f2dc-bd135bf39b6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReadCSVCorrectly\").getOrCreate()\n",
        "# Read the CSV file into a PySpark DataFrame with options to improve parsing\n",
        "try:\n",
        "    df_review= spark.read.csv(\n",
        "        csv_path,\n",
        "        header=True,\n",
        "        inferSchema=True,\n",
        "        sep=',',  # Assuming comma is the delimiter\n",
        "        quote='\"', # Assuming double quotes are used for quoting fields\n",
        "        escape='\"', # Assuming double quotes are escaped by double quotes\n",
        "        multiLine=True # Set to true if text column contains newline characters\n",
        "    )\n",
        "    print(\"CSV file loaded successfully!\")\n",
        "    df_review.show(5, truncate=False) # Display the first five rows without truncating\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "Lx7OlPqe6TN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the metadata CSV file into a PySpark DataFrame\n",
        "try:\n",
        "    df_meta = spark.read.csv(\n",
        "        meta_csv_path,\n",
        "        header=True,\n",
        "        inferSchema=True,\n",
        "        sep=',',  # Assuming comma is the delimiter\n",
        "        quote='\"', # Assuming double quotes are used for quoting fields\n",
        "        escape='\"', # Assuming double quotes are escaped by double quotes\n",
        "        multiLine=True # Set to true if text column contains newline characters\n",
        "    )\n",
        "    print(\"Meta CSV file loaded successfully!\")\n",
        "    df_meta.show(5, truncate=False) # Display the first five rows without truncating\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading meta CSV: {e}\")"
      ],
      "metadata": {
        "id": "tba1DvxPjtSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show data tpes\n",
        "df_review.printSchema()"
      ],
      "metadata": {
        "id": "WiMlxprAEUhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show summary statistics\n",
        "df_review.describe().show()"
      ],
      "metadata": {
        "id": "X0bNcdeKJ3Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.1.1**"
      ],
      "metadata": {
        "id": "oRj2lOE-raGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to count rows that are none or null\n",
        "def show_empty_rows(df):\n",
        "  # Filter rows where 'text' column is null or an empty string\n",
        "  null_or_empty_text_count = df.filter(\n",
        "    col(\"text\").isNull() | (trim(col(\"text\")) == \"\")\n",
        "  ).count()\n",
        "  return null_or_empty_text_count"
      ],
      "metadata": {
        "id": "4nA-TOEw_6yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show count of empty rows in text field before filling in with no review\n",
        "print(f\"The number of empty rows in the text column before replacing with 'no review' is: {show_empty_rows(df_review)}\")"
      ],
      "metadata": {
        "id": "jwc7vMd8CERM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace null values with \"No review\"\n",
        "df_review = df_review.fillna({'text': 'no review'})\n",
        "\n",
        "# Replace empty strings (after trimming whitespace) with \"No review\"\n",
        "df_review = df_review.withColumn(\"text\",\n",
        "    when(trim(col(\"text\")) == \"\", lit(\"no review\")).otherwise(col(\"text\"))\n",
        ")\n",
        "\n",
        "# Show the count of text column with  to verify the changes\n",
        "print(f\"The number of empty rows in the text column after replacing with no review is: {show_empty_rows(df_review)}\")\n",
        "\n",
        "# Count rows where the 'text' column is 'no review'\n",
        "no_review_count = df_review.filter(col(\"text\") == \"no review\").count()\n",
        "\n",
        "print(f\"The number of reviews with 'no review' in the text column is: {no_review_count}\")\n"
      ],
      "metadata": {
        "id": "xcXbFlC-Arya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.1.2**"
      ],
      "metadata": {
        "id": "PGG6r7-otTdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the time parser policy to legacy to handle potential parsing issues\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
        "\n",
        "# Convert 'time' from epoch milliseconds to a timestamp, then to a date string\n",
        "df_review = df_review.withColumn(\n",
        "    \"newtime\",\n",
        "    to_date(from_unixtime(col(\"time\") / 1000), \"yyyy-MM-dd\")\n",
        ")\n",
        "\n",
        "# Display the first few rows with the new column\n",
        "df_review.select(\"user_id\",\"name\",\"rating\",\"time\", \"newtime\",\"text\",\"gmap_id\").show(5)"
      ],
      "metadata": {
        "id": "e702ILlLH2oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2**"
      ],
      "metadata": {
        "id": "s9SOA54GxrVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2.1**"
      ],
      "metadata": {
        "id": "73LlzorL555T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of reviews per unique gmap_id\n",
        "reviews_per_gmap = df_review.groupBy(\"gmap_id\").agg(count(\"*\").alias(\"review_count\"))\n",
        "\n",
        "# Cast the review_count to float type\n",
        "reviews_per_gmap = reviews_per_gmap.withColumn(\"review_count\", col(\"review_count\").cast(DoubleType()))\n",
        "\n",
        "# Show the top 5 results\n",
        "print(\"Number of reviews per unique gmap_id (Top 5):\")\n",
        "reviews_per_gmap.orderBy(col(\"review_count\").desc()).show(5)"
      ],
      "metadata": {
        "id": "aPzG0khxewGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2.2**"
      ],
      "metadata": {
        "id": "smGy0nJf6Akh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert PySpark DataFrame to pandas DataFrame\n",
        "df = df_review.toPandas()\n",
        "\n",
        "# Create 'review_time' column at the hour level\n",
        "df['review_time'] = pd.to_datetime(df['time'], unit='ms').dt.hour\n",
        "\n",
        "# Display the first 5 rows of the pandas DataFrame\n",
        "print(\"Pandas DataFrame with 'review_time' column (Top 5):\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "GKKiQDDB4JRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.2.3**"
      ],
      "metadata": {
        "id": "br0XEAsR6DPr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2082982c"
      },
      "source": [
        "# Analyze the distribution of reviews by hour\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='review_time', palette='viridis')\n",
        "plt.title('Distribution of Review Times by Hour')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "\n",
        "# Analyze the number of businesses reviewed at different times\n",
        "unique_gmap_ids_per_hour = df.groupby('review_time')['gmap_id'].nunique().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=unique_gmap_ids_per_hour, x='review_time', y='gmap_id', palette='viridis')\n",
        "plt.title('Number of Unique Businesses Reviewed per Hour')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Number of Unique Businesses')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe1c49ca"
      },
      "source": [
        "### Insights from Visualizations\n",
        "\n",
        "The visualizations above provide insights into the review patterns based on the time of day:\n",
        "\n",
        "**Distribution of Review Times by Hour:**\n",
        "- The first plot shows the total number of reviews submitted at each hour of the day.\n",
        "- We can observe that there is a peak in reviews during certain hours, indicating when users are most active in leaving feedback. This could be related to business operating hours, user routines, or other factors.\n",
        "\n",
        "**Number of Unique Businesses Reviewed per Hour:**\n",
        "- The second plot shows the number of distinct businesses (identified by `gmap_id`) that received at least one review during each hour.\n",
        "- This helps understand if reviews are spread across many businesses throughout the day or concentrated on a few during specific times.\n",
        "\n",
        "Further analysis could involve looking at the average rating per hour, the types of businesses reviewed at different times, or correlating review times with other factors in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3**"
      ],
      "metadata": {
        "id": "QdkOlMSFENva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3.1**"
      ],
      "metadata": {
        "id": "UtIiytTrERdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the second CSV file\n",
        "meta_csv_path = '/content/drive/My Drive/SIT742/meta-review-business.csv'\n",
        "\n",
        "# Read the second CSV file into a PySpark DataFrame\n",
        "try:\n",
        "    df_meta = spark.read.csv(\n",
        "        meta_csv_path,\n",
        "        header=True,\n",
        "        inferSchema=True,\n",
        "        sep=',',  # Assuming comma is the delimiter\n",
        "        quote='\"', # Assuming double quotes are used for quoting fields\n",
        "        escape='\"', # Assuming double quotes are escaped by double quotes\n",
        "        multiLine=True # Set to true if text column contains newline characters\n",
        "    )\n",
        "    print(\"Meta CSV file loaded successfully!\")\n",
        "    df_meta.show(5, truncate=False) # Display the first five rows without truncating\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading meta CSV: {e}\")"
      ],
      "metadata": {
        "id": "DIFxrO7WBpZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3.2**"
      ],
      "metadata": {
        "id": "7-wiUG-yEYYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the two DataFrames on the 'gmap_id' column\n",
        "joined_df = df_review.join(df_meta, on=\"gmap_id\", how=\"inner\")\n",
        "\n",
        "# Show the schema and some rows of the joined DataFrame\n",
        "print(\"Joined DataFrame Schema:\")\n",
        "joined_df.printSchema()\n",
        "\n",
        "print(\"Joined DataFrame (Top 5):\")\n",
        "joined_df.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "480yoGtO52Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the day of the week from the 'newtime' column\n",
        "joined_df_with_day = joined_df.withColumn(\"day_of_week\", date_format(col(\"newtime\"), \"E\"))\n",
        "\n",
        "# Group by day of the week and count the reviews\n",
        "reviews_by_day = joined_df_with_day.groupBy(\"day_of_week\").count()\n",
        "\n",
        "# Define the order of the days of the week for plotting\n",
        "day_order = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
        "\n",
        "# Convert to pandas DataFrame for plotting\n",
        "reviews_by_day_pd = reviews_by_day.toPandas()\n",
        "\n",
        "# Sort the pandas DataFrame by the defined day order\n",
        "reviews_by_day_pd['day_of_week'] = pd.Categorical(reviews_by_day_pd['day_of_week'], categories=day_order, ordered=True)\n",
        "reviews_by_day_pd = reviews_by_day_pd.sort_values('day_of_week')\n",
        "\n",
        "\n",
        "# Plot the results as a line chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=reviews_by_day_pd, x='day_of_week', y='count', marker='o')\n",
        "plt.title('Number of Reviews per Day of the Week (Joined Data)')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w54ao2_eCUEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the day with the maximum number of reviews\n",
        "peak_day = reviews_by_day_pd.loc[reviews_by_day_pd['count'].idxmax()]['day_of_week']\n",
        "print(f\"The workday with the most reviews is: {peak_day}\")\n",
        "\n",
        "# Rename the 'name' column in df_meta to avoid ambiguity\n",
        "df_meta_renamed = df_meta.withColumnRenamed(\"name\", \"business_name\")\n",
        "\n",
        "# Re-join the dataframes with the renamed column\n",
        "# Ensure the 'newtime' column and 'day_of_week' are still available after re-joining\n",
        "\n",
        "joined_df_renamed = df_review.join(df_meta_renamed, on=\"gmap_id\", how=\"inner\")\n",
        "joined_df_with_day = joined_df_renamed.withColumn(\"day_of_week\", date_format(col(\"newtime\"), \"E\"))\n",
        "\n",
        "\n",
        "# Filter the joined DataFrame for the peak day\n",
        "peak_day_df = joined_df_with_day.filter(col(\"day_of_week\") == peak_day)\n",
        "\n",
        "# Group by business name and category and calculate the average rating\n",
        "avg_rating_by_business = peak_day_df.groupBy(\"business_name\", \"category\").agg(avg(\"rating\").alias(\"average_rating\"))\n",
        "\n",
        "# Find the business(es) with the highest average rating on the peak day\n",
        "highest_rated_businesses = avg_rating_by_business.orderBy(col(\"average_rating\").desc()).limit(5)\n",
        "\n",
        "print(f\"\\nBusinesses with the highest average rating on {peak_day} (Top 5):\")\n",
        "highest_rated_businesses.show(truncate=False)"
      ],
      "metadata": {
        "id": "Oliw0wxTDMIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.3.3**"
      ],
      "metadata": {
        "id": "lyKfEILCEeiL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0549d5b"
      },
      "source": [
        "# Group by category and count the number of businesses\n",
        "category_counts = joined_df_renamed.groupBy(\"category\").count()\n",
        "\n",
        "# Order by count in descending order\n",
        "category_counts = category_counts.orderBy(col(\"count\").desc())\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "category_counts_pd = category_counts.toPandas()\n",
        "\n",
        "# Select the top 10 categories\n",
        "top_10_categories = category_counts_pd.head(10)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(data=top_10_categories, x='category', y='count', palette='viridis')\n",
        "plt.title('Top 10 Business Categories by Count')\n",
        "plt.xlabel('Business Category')\n",
        "plt.ylabel('Number of Businesses')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e45abe92"
      },
      "source": [
        "# Get the list of top 10 categories\n",
        "top_categories_list = top_10_categories['category'].tolist()\n",
        "\n",
        "# Filter the joined DataFrame to include only the top 10 categories\n",
        "filtered_df = joined_df_with_day.filter(col(\"category\").isin(top_categories_list))\n",
        "\n",
        "# Extract the hour of the day from the 'newtime' column\n",
        "filtered_df = filtered_df.withColumn(\"review_hour\", date_format(col(\"newtime\"), \"H\"))\n",
        "\n",
        "# Show the schema and some rows of the filtered DataFrame with the new column\n",
        "print(\"Filtered DataFrame Schema:\")\n",
        "filtered_df.printSchema()\n",
        "\n",
        "print(\"Filtered DataFrame (Top 5):\")\n",
        "filtered_df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ece5d5c"
      },
      "source": [
        "# Group by category and review_hour and count the number of reviews\n",
        "reviews_by_category_hour = filtered_df.groupBy(\"category\", \"review_hour\").count()\n",
        "\n",
        "# Find the peak review hour for each category\n",
        "# This requires window functions to partition by category and order by count\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "window_spec = Window.partitionBy(\"category\").orderBy(col(\"count\").desc())\n",
        "\n",
        "ranked_reviews = reviews_by_category_hour.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "peak_hours_per_category = ranked_reviews.filter(col(\"rank\") == 1)\n",
        "\n",
        "# Display the peak review hour for each of the top 10 categories\n",
        "print(\"Peak review hour for each of the top 10 categories:\")\n",
        "peak_hours_per_category.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56150fc3"
      },
      "source": [
        "## Analyze average ratings by category\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb3b8c25"
      },
      "source": [
        "# Group by category and calculate the average rating\n",
        "avg_rating_by_category = joined_df_renamed.groupBy(\"category\").agg(avg(\"rating\").alias(\"average_rating\"))\n",
        "\n",
        "# Order by average_rating in descending order\n",
        "avg_rating_by_category = avg_rating_by_category.orderBy(col(\"average_rating\").desc())\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "avg_rating_by_category_pd = avg_rating_by_category.toPandas()\n",
        "\n",
        "# Select the top 10 categories by average rating\n",
        "top_10_avg_rating_categories = avg_rating_by_category_pd.head(10)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(data=top_10_avg_rating_categories, x='category', y='average_rating', palette='viridis')\n",
        "plt.title('Top 10 Business Categories by Average Rating')\n",
        "plt.xlabel('Business Category')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ac079f"
      },
      "source": [
        "**Analysis Summary:**\n",
        "\n",
        "Business Categories, Peak Hours, and Average Ratings\n",
        "This analysis explored review patterns based on business categories, focusing on the distribution of businesses, peak review hours for top categories, and average ratings across categories.\n",
        "\n",
        "Distribution of Businesses Across Categories:\n",
        "\n",
        "Based on the analysis of business categories, the most frequent categories in the dataset are:\n",
        "\n",
        "['Shopping mall']\n",
        "['Fast food restaurant', 'Breakfast restaurant', 'Hamburger restaurant', 'Restaurant', 'Sandwich shop']\n",
        "['Department store', 'Clothing store', 'Craft store', 'Shoe store', 'Sporting goods store']\n",
        "['Grocery store', 'Grocery delivery service']\n",
        "['Mexican restaurant']\n",
        "['Fast food restaurant', 'Breakfast restaurant', 'Burger joint', 'Restaurant', 'Sandwich shop']\n",
        "['Restaurant']\n",
        "['Warehouse store', 'Department store']\n",
        "['American restaurant']\n",
        "['Gas station']\n",
        "The bar plot titled \"Top 10 Business Categories by Count\" visually represents this distribution, clearly showing the categories with the highest number of associated businesses.\n",
        "\n",
        "Peak Review Hours for Top Categories:\n",
        "\n",
        "The analysis of peak review hours for the top 10 business categories revealed that for all of these categories, the hour with the highest number of reviews is '0' (which likely corresponds to midnight or the beginning of the day, depending on the exact time data representation).\n",
        "\n",
        "Average Ratings by Category:\n",
        "\n",
        "The calculation of average ratings by category identified the categories with the highest average ratings. The top 10 categories by average rating are:\n",
        "\n",
        "['Beauty salon', 'Hair care', 'Hair salon', 'Nail salon']\n",
        "['Snowboard shop', 'Clothing store', 'Shoe store', 'Ski shop', 'Sporting goods store', 'Store']\n",
        "['Horseback riding service', 'Bed & breakfast', 'Cabin rental agency', 'Cottage', 'Guest house']\n",
        "['Pediatric dentist', 'Dentist']\n",
        "['Pediatrician']\n",
        "['Surgeon', 'Doctor', 'Medical spa']\n",
        "['Waterfall', 'Tourist attraction']\n",
        "['Rafting', 'Raft trip outfitter', 'Scenic spot']\n",
        "['Dogsled ride service', 'Helicopter tour agency', 'Tour operator']\n",
        "['Leather goods store', 'Store']\n",
        "The bar plot titled \"Top 10 Business Categories by Average Rating\" illustrates these top-rated categories and their corresponding average ratings.\n",
        "\n",
        "Visualizations Summary:\n",
        "\n",
        "Distribution of Review Times by Hour: A bar plot showing the total number of reviews submitted at each hour of the day, highlighting peak review hours across all businesses.\n",
        "Number of Unique Businesses Reviewed per Hour: A bar plot illustrating the number of distinct businesses that received reviews during each hour, indicating the spread of review activity throughout the day.\n",
        "Number of Reviews per Day of the Week (Joined Data): A line plot showing the trend of review counts across the days of the week, revealing which days have the highest review activity.\n",
        "Top 10 Business Categories by Count: A bar plot displaying the frequency of the most common business categories in the dataset.\n",
        "Top 10 Business Categories by Average Rating: A bar plot showcasing the average ratings for the categories with the highest overall sentiment.\n",
        "These visualizations collectively provide a comprehensive overview of review patterns in the dataset, highlighting popular business categories, peak review times, and categories with the most favorable ratings."
      ]
    }
  ]
}